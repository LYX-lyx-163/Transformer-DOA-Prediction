{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14808247,"sourceType":"datasetVersion","datasetId":9469034}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:31:35.570529Z","iopub.execute_input":"2026-02-11T14:31:35.570914Z","iopub.status.idle":"2026-02-11T14:31:35.579924Z","shell.execute_reply.started":"2026-02-11T14:31:35.570883Z","shell.execute_reply":"2026-02-11T14:31:35.578808Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nbp174/training_data_no_bp.npz\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"%%writefile tranlstm.py\nimport torch\nimport math\nfrom torch.nn.modules import Transformer, TransformerEncoder, TransformerEncoderLayer, LayerNorm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Dict, Tuple, Optional, List\nimport copy\n\n# ---------- 在这里直接定义掩码生成函数（替代原外部导入）----------\ndef generate_square_subsequent_mask(sz: int):\n    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\n# ---------- 以下是你的原始 tranlstm.py 全部内容（只删除了那行 import）----------\nclass MyTransformer(torch.nn.Module):\n    def __init__(\n            self,\n            device,\n            number_time_series: int,\n            seq_length: int = 48,\n            output_seq_len: int = None,\n            d_model: int = 128,\n            n_heads: int = 8,\n            dropout=0.1,\n            forward_dim=2048,\n            sigmoid=False):\n        super().__init__()\n        if output_seq_len is None:\n            output_seq_len = seq_length\n        self.out_seq_len = output_seq_len\n        self.mask = generate_square_subsequent_mask(seq_length).to(device)\n        self.dense_shape = torch.nn.Linear(number_time_series, d_model)\n        self.dense_t = torch.nn.Linear(1, d_model)\n        self.pe = SimplePositionalEncoding(d_model)\n        self.transformer = Transformer(\n            d_model,\n            nhead=n_heads,\n            num_encoder_layers=1,\n            num_decoder_layers=1,\n            device=device,\n            activation=torch.nn.functional.selu,\n            dropout=0)\n        self.final_layer = nn.Linear(d_model, 1)\n        self.permute_layer = nn.Linear(seq_length, 1)\n        self.sequence_size = seq_length\n        self.tgt_mask = generate_square_subsequent_mask(seq_length).to(device)\n\n    def forward(self, x, t, tgt_mask=None):\n        x = self.encode_sequence(x, src_mask=self.mask)\n        x = self.decode_seq(x, t, tgt_mask)\n        x = x.permute(2, 1, 0)\n        x = self.permute_layer(x)\n        return x\n\n    def basic_feature(self, x: torch.Tensor):\n        x = self.dense_shape(x)\n        x = self.pe(x)\n        x = x.permute(1, 0, 2)\n        return x\n\n    def basic_t(self, x: torch.Tensor):\n        x = self.dense_t(x)\n        x = self.pe(x)\n        x = x.permute(1, 0, 2)\n        return x\n\n    def encode_sequence(self, x, src_mask=None):\n        x = self.basic_feature(x)\n        x = self.transformer.encoder(src=x, mask=src_mask)\n        return x\n\n    def decode_seq(self, mem, t, tgt_mask=None, view_number=None) -> torch.Tensor:\n        if view_number is None:\n            view_number = self.out_seq_len\n        if tgt_mask is None:\n            tgt_mask = self.tgt_mask\n        t = self.basic_t(t)\n        x = self.transformer.decoder(t, mem, tgt_mask=tgt_mask)\n        x = self.final_layer(x)\n        return x\n\nclass GLU(nn.Module):\n    def __init__(self, input_dim: int, output_dim: int):\n        super(GLU, self).__init__()\n        self.fc = nn.Linear(input_dim, output_dim*2)\n    def forward(self, x):\n        x = F.glu(self.fc(x))\n        return x\n\nclass GRN(nn.Module):\n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int,\n                 dropout: Optional[float] = 0.05,\n                 context_dim: Optional[int] = None):\n        super().__init__()\n        self.layernorm = nn.LayerNorm(output_dim)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        if context_dim is not None:\n            self.context_fc = nn.Linear(context_dim, hidden_dim)\n        self.skip = nn.Linear(input_dim, output_dim)\n        self.forw = nn.Sequential(\n            nn.ELU(),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Dropout(dropout),\n            GLU(input_dim=output_dim, output_dim=output_dim))\n    def forward(self, x, c: Optional[torch.Tensor] = None):\n        res = self.skip(x)\n        x = self.fc1(x)\n        if c is not None:\n            c = self.context_fc(c)\n            x += c\n        x = self.forw(x) + res\n        x = self.layernorm(x)\n        return x\n\nclass VariableSelectionNetwork(nn.Module):\n    def __init__(self,\n                 input_dim: int,\n                 input_num: int,\n                 hidden_dim: int,\n                 dropout: float,\n                 context_dim: Optional[int] = None):\n        super(VariableSelectionNetwork, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.input_dim = input_dim\n        self.input_num = input_num\n        self.dropout = dropout\n        self.context_dim = context_dim\n        self.flattened_grn = GRN(\n            input_dim=self.input_num * self.input_dim,\n            hidden_dim=self.hidden_dim,\n            output_dim=self.input_num,\n            dropout=self.dropout,\n            context_dim=self.context_dim)\n        self.softmax = nn.Softmax(dim=1)\n        self.single_variable_grns = nn.ModuleList()\n        for _ in range(self.input_num):\n            self.single_variable_grns.append(\n                GRN(input_dim=self.input_dim,\n                    hidden_dim=self.hidden_dim,\n                    output_dim=self.hidden_dim,\n                    dropout=self.dropout,\n                    context_dim=context_dim))\n    def forward(self, flattened_embedding, context=None):\n        x = flattened_embedding.reshape(flattened_embedding.shape[0], -1)\n        sparse_weights = self.flattened_grn(x, context)\n        sparse_weights = self.softmax(sparse_weights).unsqueeze(2)\n        processed_inputs = []\n        for i in range(self.input_num):\n            processed_inputs.append(\n                self.single_variable_grns[i](x[..., (i * self.input_dim): (i + 1) * self.input_dim], context))\n        processed_inputs = torch.stack(processed_inputs, dim=-1)\n        outputs = processed_inputs * sparse_weights.transpose(1, 2)\n        return outputs, sparse_weights\n\nclass StaticCovariateEncoder(nn.Module):\n    def __init__(self, f_dim, hidden_dim):\n        super(StaticCovariateEncoder, self).__init__()\n        self.f_dim = f_dim\n        self.vsn = VariableSelectionNetwork(\n            input_dim=hidden_dim,\n            input_num=4,\n            hidden_dim=hidden_dim,\n            dropout=0.1,\n            context_dim=hidden_dim,\n        )\n        self.grn = nn.ModuleList([GRN(\n            input_dim=hidden_dim,\n            hidden_dim=hidden_dim,\n            output_dim=hidden_dim,) for _ in range(4)])\n    def forward(self, x):\n        variable_ctx, sparse_weights = self.vsn(x)\n        variable_ctx = variable_ctx.sum(axis=-1)\n        cs, ce, cc, ch = tuple(m(variable_ctx) for m in self.grn)\n        return cs, ce, cc, ch\n\nclass RnnEecoder(nn.Module):\n    def __init__(self, f, n):\n        super().__init__()\n        self.f = f\n        self.n = n\n        self.lstm = nn.LSTM(1, self.n, batch_first=True)\n        self.gate = GLU(self.n, self.n)\n        self.fc = nn.Linear(f, n)\n        self.layernorm = nn.LayerNorm(n, eps=1e-5)\n    def forward(self, x: torch.Tensor, c_h: torch.Tensor, c_c: torch.Tensor):\n        res = x\n        if self.f != self.n:\n            res = self.fc(res)\n        h0 = (c_h.unsqueeze(0), c_c.unsqueeze(0))\n        x, (hn, cn) = self.lstm(x, hx=h0)\n        x = self.layernorm(self.gate(x) + res)\n        return x\n\nclass InterpretableMultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        assert config.hidden_dim % config.n_head == 0\n        self.i = 0\n        self.d_head = config.hidden_dim // config.n_head\n        self.qkv_linears = nn.Linear(config.hidden_dim, (2 * self.n_head + 1) * self.d_head, bias=True)\n        self.out_proj = nn.Linear(self.d_head, config.hidden_dim, bias=True)\n        self.attn_dropout = nn.Dropout(config.attn_dropout)\n        self.out_dropout = nn.Dropout(config.dropout)\n        self.scale = self.d_head**-0.5\n        self.register_buffer(\"_mask\", torch.triu(torch.full((config.example_length, config.example_length), float('-inf')), 1).unsqueeze(0))\n    def forward(self, x: torch.Tensor, mask_future_timesteps: bool = True) -> Tuple[Tensor, Tensor]:\n        bs, t, h_size = x.shape\n        qkv = self.qkv_linears(x)\n        q, k, v = qkv.split((self.n_head * self.d_head, self.n_head * self.d_head, self.d_head), dim=-1)\n        q = q.view(bs, t, self.n_head, self.d_head)\n        k = k.view(bs, t, self.n_head, self.d_head)\n        v = v.view(bs, t, self.d_head)\n        attn_score = torch.matmul(q.permute((0, 2, 1, 3)), k.permute((0, 2, 3, 1)))\n        attn_score.mul_(self.scale)\n        if mask_future_timesteps:\n            attn_score = attn_score + self._mask\n        attn_prob = F.softmax(attn_score, dim=3)\n        attn_prob = self.attn_dropout(attn_prob)\n        attn_vec = torch.matmul(attn_prob, v.unsqueeze(1))\n        m_attn_vec = torch.mean(attn_vec, dim=1)\n        out = self.out_proj(m_attn_vec)\n        out = self.out_dropout(out)\n        return out, attn_vec\n\nclass TemporalFusionDecoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, context_dim, output_dim, config):\n        super().__init__()\n        self.example_length = config.example_length\n        self.enrichment_grn = GRN(\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            output_dim=output_dim,\n            dropout=0.05,\n            context_dim=context_dim\n        )\n        self.attention = InterpretableMultiHeadAttention(config)\n        self.att_gate = GLU(config.hidden_dim, config.hidden_dim)\n        self.attention_ln = LayerNorm(config.hidden_dim, eps=1e-3)\n        self.positionwise_grn = GRN(config.hidden_dim,\n                                    config.hidden_dim,\n                                    dropout=config.dropout,\n                                    output_dim=config.hidden_dim)\n        self.decoder_gate = GLU(config.hidden_dim, config.hidden_dim)\n        self.decoder_ln = LayerNorm(config.hidden_dim, eps=1e-3)\n    def forward(self, x, ce):\n        res = x[:, -1, :]\n        ce = ce.unsqueeze(1).repeat(1, self.example_length, 1)\n        enriched = self.enrichment_grn(x, c=ce)\n        x, _ = self.attention(enriched)\n        x = x[:, -1, :]\n        enriched = enriched[:, -1, :]\n        x = self.att_gate(x)\n        x = x + enriched\n        x = self.attention_ln(x)\n        x = self.positionwise_grn(x)\n        x = self.decoder_gate(x)\n        x = x + res\n        x = self.decoder_ln(x)\n        return x\n\nclass TemporalFusionTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.static_encoder1 = VariableSelectionNetwork(\n                input_dim=config.static_dim,\n                input_num=config.static_num,\n                hidden_dim=config.hidden_dim,\n                dropout=0.1)\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim, config.hidden_dim)\n        self.varialbe_selection = VariableSelectionNetwork(\n            input_dim=config.temporal_dim,\n            input_num=config.example_length,\n            hidden_dim=config.example_length,\n            dropout=0.1,\n            context_dim=config.hidden_dim,\n        )\n        self.rnn_encoder = RnnEecoder(1, config.hidden_dim)\n        self.time_decoder = TemporalFusionDecoder(config.hidden_dim, config.hidden_dim, config.hidden_dim, config)\n    def forward(self, x, b):\n        b, _ = self.static_encoder1(b)\n        cs, ce, ch, cc = self.static_encoder2(b)\n        x, _ = self.varialbe_selection(x, cs)\n        x = x.unsqueeze(-1)\n        x = self.rnn_encoder(x, ch, cc)\n        x = self.time_decoder(x, ce)\n        return x\n\nclass LstmVsn(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.input_dim = 3\n        self.memory_cell = 8\n        self.body_dim = 4\n        self.n = 16\n        config.hidden_dim = self.input_dim*self.memory_cell\n        self.lstm1 = nn.LSTM(1, 8, batch_first=True)\n        self.lstm2 = nn.LSTM(1, 8, batch_first=True)\n        self.lstm3 = nn.LSTM(1, 8, batch_first=True)\n        self.fusion_lstm = nn.LSTM(24, 24, batch_first=True)\n        self.static_encoder1 = VariableSelectionNetwork(\n                input_dim=config.static_dim,\n                input_num=config.static_num,\n                hidden_dim=config.hidden_dim,\n                dropout=0.1)\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim, config.hidden_dim)\n        self.time_decoder = TemporalFusionDecoder(\n            input_dim=config.hidden_dim,\n            hidden_dim=config.hidden_dim,\n            context_dim=config.hidden_dim,\n            output_dim=config.hidden_dim,\n            config=config)\n        self.fc1 = nn.Linear(8, 24)\n        self.fc2 = nn.Linear(8, 24)\n        self.fc3 = nn.Linear(8, 24)\n        self.bottle_neck = nn.Sequential(\n            nn.Linear(config.hidden_dim*4, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n    def forward(self, x, b):\n        b, _ = self.static_encoder1(b)\n        cs, ce, ch, cc = self.static_encoder2(b)\n        x1, (hn, cn) = self.lstm1(x[..., 0].unsqueeze(-1))\n        x2, (hn, cn) = self.lstm2(x[..., 1].unsqueeze(-1))\n        x3, (hn, cn) = self.lstm3(x[..., 2].unsqueeze(-1))\n        x = torch.cat((x1, x2, x3), dim=-1)\n        x, (hn, cn) = self.fusion_lstm(x)\n        x = self.time_decoder(x, cs)\n        x1 = self.fc1(x1[:, -1])\n        x2 = self.fc2(x2[:, -1])\n        x3 = self.fc3(x3[:, -1])\n        x = torch.cat((x, x1, x2, x3), dim=-1)\n        x = self.bottle_neck(x)\n        return x\n\nclass LstmVsn2(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.input_dim = 3\n        self.memory_cell = 8\n        self.body_dim = 4\n        self.n = 16\n        self.lstm1 = nn.LSTM(1, self.memory_cell, batch_first=True)\n        self.lstm2 = nn.LSTM(1, self.memory_cell, batch_first=True)\n        self.lstm3 = nn.LSTM(1, self.memory_cell, batch_first=True)\n        config.hidden_dim = self.input_dim*self.memory_cell\n        self.static_encoder1 = VariableSelectionNetwork(\n                input_dim=config.static_dim,\n                input_num=config.static_num,\n                hidden_dim=config.hidden_dim,\n                dropout=0.1)\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim, config.hidden_dim)\n        self.time_decoder = TemporalFusionDecoder(config.hidden_dim, config.hidden_dim, config.hidden_dim, config)\n        self.fc1 = nn.Linear(8, 1)\n        self.fc2 = nn.Linear(8, 1)\n        self.fc3 = nn.Linear(8, 1)\n        self.skip_fc = nn.Linear(self.input_dim+1, 1)\n    def forward(self, x, b):\n        b, _ = self.static_encoder1(b)\n        cs, ce, ch, cc = self.static_encoder2(b)\n        x1, (hn, cn) = self.lstm1(x[..., 0].unsqueeze(-1))\n        x2, (hn, cn) = self.lstm2(x[..., 1].unsqueeze(-1))\n        x3, (hn, cn) = self.lstm3(x[..., 2].unsqueeze(-1))\n        x = torch.cat((x1, x2, x3), dim=2)\n        x = self.time_decoder(x, ce)\n        x1 = self.fc1(x1[:, -1])\n        x2 = self.fc2(x2[:, -1])\n        x3 = self.fc3(x3[:, -1])\n        x = torch.cat((x, x1, x2, x3), dim=1)\n        x = self.skip_fc(x)\n        return x\n\nclass SimplePositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(SimplePositionalEncoding, self).__init__()\n        self.dropout = torch.nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\ndef weights_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.constant_(m.bias, 0)\n    elif isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:31:35.672499Z","iopub.execute_input":"2026-02-11T14:31:35.673294Z","iopub.status.idle":"2026-02-11T14:31:35.693381Z","shell.execute_reply.started":"2026-02-11T14:31:35.673249Z","shell.execute_reply":"2026-02-11T14:31:35.692135Z"}},"outputs":[{"name":"stdout","text":"Overwriting tranlstm.py\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%writefile rass_loader.py\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass RASSDataset(Dataset):\n    def __init__(self, npz_path):\n        data = np.load(npz_path)\n        X = data['X']   # (N, 70)\n        y = data['y']   # (N,)\n        # 前66维 -> (N,6,11), 后4维 -> (N,4)\n        temporal = X[:, :66].reshape(-1, 6, 11)\n        static   = X[:, 66:]\n        self.temporal = torch.FloatTensor(temporal)\n        self.static   = torch.FloatTensor(static)\n        self.target   = torch.FloatTensor(y).view(-1, 1)\n\n    def __len__(self):\n        return len(self.target)\n\n    def __getitem__(self, idx):\n        return {\n            'temporal': self.temporal[idx],\n            'static':   self.static[idx],\n            'target':   self.target[idx]\n        }\n\ndef create_dataloader(npz_path, batch_size=32, shuffle=True):\n    dataset = RASSDataset(npz_path)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:31:35.695352Z","iopub.execute_input":"2026-02-11T14:31:35.695755Z","iopub.status.idle":"2026-02-11T14:31:35.718860Z","shell.execute_reply.started":"2026-02-11T14:31:35.695714Z","shell.execute_reply":"2026-02-11T14:31:35.717818Z"}},"outputs":[{"name":"stdout","text":"Writing rass_loader.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"%%writefile rass_model.py\nimport torch\nimport torch.nn as nn\nfrom torch.nn import LayerNorm\n# 从刚才修好的 tranlstm.py 导入需要的模块\nfrom tranlstm import VariableSelectionNetwork, StaticCovariateEncoder, GRN, GLU, InterpretableMultiHeadAttention, TemporalFusionDecoder\n\nclass RASSTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # 静态编码器（完全复用）\n        self.static_encoder1 = VariableSelectionNetwork(\n            input_dim=config.static_dim,\n            input_num=config.static_num,\n            hidden_dim=config.hidden_dim,\n            dropout=0.1\n        )\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim, config.hidden_dim)\n\n        # 时序编码：单LSTM + 投影\n        self.lstm = nn.LSTM(\n            input_size=11,\n            hidden_size=64,\n            batch_first=True\n        )\n        self.lstm_proj = nn.Linear(64, config.hidden_dim)\n\n        # 时序融合解码器（完全复用）\n        self.time_decoder = TemporalFusionDecoder(\n            input_dim=config.hidden_dim,\n            hidden_dim=config.hidden_dim,\n            context_dim=config.hidden_dim,\n            output_dim=config.hidden_dim,\n            config=config\n        )\n\n        # 输出层\n        self.output_layer = nn.Linear(config.hidden_dim, 1)\n\n    def forward(self, temporal, static):\n        # 静态编码\n        static_out, _ = self.static_encoder1(static)\n        cs, ce, ch, cc = self.static_encoder2(static_out)\n\n        # 时序编码\n        lstm_out, _ = self.lstm(temporal)          # (batch, 6, 64)\n        lstm_out = self.lstm_proj(lstm_out)       # (batch, 6, hidden_dim)\n\n        # 解码（context 使用 ce）\n        decoder_out = self.time_decoder(lstm_out, ce)  # (batch, hidden_dim)\n\n        # 预测\n        pred = self.output_layer(decoder_out)      # (batch, 1)\n        return pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:31:35.720131Z","iopub.execute_input":"2026-02-11T14:31:35.720463Z","iopub.status.idle":"2026-02-11T14:31:35.738746Z","shell.execute_reply.started":"2026-02-11T14:31:35.720434Z","shell.execute_reply":"2026-02-11T14:31:35.737759Z"}},"outputs":[{"name":"stdout","text":"Writing rass_model.py\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 导入我们刚写的 loader 和 model\nfrom rass_loader import create_dataloader\nfrom rass_model import RASSTransformer\n\n# ---------- 配置类 ----------\nclass Config:\n    def __init__(self):\n        self.hidden_dim = 64\n        self.example_length = 6\n        self.static_dim = 4\n        self.static_num = 4\n        self.temporal_dim = 11\n        self.n_head = 4          # hidden_dim 必须能被 n_head 整除\n        self.dropout = 0.1\n        self.attn_dropout = 0.1\n\nconfig = Config()\n\n# ---------- 修改为你的 .npz 文件路径 ----------\nnpz_path = '/kaggle/input/nbp无血压版174个小样本/training_data_no_bp.npz'  # 如果不对，运行下面一行先查看\n# !ls /kaggle/input/nbp无血压版174个小样本/   # 取消注释查看文件夹内容，确认文件名\n\n# ---------- 加载数据 ----------\ntrain_loader = create_dataloader(npz_path, batch_size=32, shuffle=True)\n\n# 检查一个 batch\nbatch = next(iter(train_loader))\nprint(f\"temporal shape: {batch['temporal'].shape}\")\nprint(f\"static shape:   {batch['static'].shape}\")\nprint(f\"target shape:   {batch['target'].shape}\")\n\n# ---------- 设备 ----------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# ---------- 模型、优化器、损失 ----------\nmodel = RASSTransformer(config).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# ---------- 训练循环 ----------\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        x_temporal = batch['temporal'].to(device)\n        x_static   = batch['static'].to(device)\n        y_true     = batch['target'].to(device)\n\n        optimizer.zero_grad()\n        y_pred = model(x_temporal, x_static)\n        loss = criterion(y_pred, y_true)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * len(y_true)\n\n    avg_loss = total_loss / len(train_loader.dataset)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n# ---------- 绘制损失曲线 ----------\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\n# ---------- 保存模型 ----------\ntorch.save(model.state_dict(), 'rass_transformer.pth')\nprint(\"Model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:31:35.830405Z","iopub.execute_input":"2026-02-11T14:31:35.830755Z","iopub.status.idle":"2026-02-11T14:31:35.852848Z","shell.execute_reply.started":"2026-02-11T14:31:35.830726Z","shell.execute_reply":"2026-02-11T14:31:35.851426Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/25264736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# ---------- 加载数据 ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# 检查一个 batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/rass_loader.py\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[0;34m(npz_path, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRASSDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/rass_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, npz_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRASSDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpz_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# (N, 70)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# (N,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/nbp无血压版174个小样本/training_data_no_bp.npz'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/nbp无血压版174个小样本/training_data_no_bp.npz'","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass RASSDataset(Dataset):\n    \"\"\"从 .npz 加载数据，输出 (X_temporal, X_static, y)\"\"\"\n    def __init__(self, npz_path):\n        data = np.load(npz_path)\n        X = data['X']   # shape (N, 70)\n        y = data['y']   # shape (N,)\n\n        # 你的特征排列：前66维是时序特征（6时间步 × 11特征），后4维是静态特征\n        temporal = X[:, :66].reshape(-1, 6, 11)   # (N, 6, 11)\n        static   = X[:, 66:]                     # (N, 4)\n\n        self.temporal = torch.FloatTensor(temporal)\n        self.static   = torch.FloatTensor(static)\n        self.target   = torch.FloatTensor(y).view(-1, 1)\n\n    def __len__(self):\n        return len(self.target)\n\n    def __getitem__(self, idx):\n        return {\n            'temporal': self.temporal[idx],   # (6, 11)\n            'static':   self.static[idx],     # (4,)\n            'target':   self.target[idx]      # (1,)\n        }\n\ndef create_dataloader(npz_path, batch_size=32, shuffle=True):\n    dataset = RASSDataset(npz_path)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:31:35.853551Z","iopub.status.idle":"2026-02-11T14:31:35.853879Z","shell.execute_reply.started":"2026-02-11T14:31:35.853744Z","shell.execute_reply":"2026-02-11T14:31:35.853762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import LayerNorm\n# 假设原 tranlstm.py 在同一目录下，或者你已经把文件上传到了 Kaggle\nfrom tranlstm import VariableSelectionNetwork, StaticCovariateEncoder, GRN, GLU, InterpretableMultiHeadAttention, TemporalFusionDecoder\n\nclass RASSTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # ---------- 1. 静态特征编码（完全复用原代码）----------\n        # config.static_dim 应为4，config.static_num 应为4\n        self.static_encoder1 = VariableSelectionNetwork(\n            input_dim=config.static_dim,\n            input_num=config.static_num,\n            hidden_dim=config.hidden_dim,\n            dropout=0.1\n        )\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim, config.hidden_dim)\n\n        # ---------- 2. 时序编码：单个LSTM处理11维特征 ----------\n        self.input_dim = 11          # 你的动态特征数\n        self.seq_len = 6            # 时间步长\n        self.lstm_hidden = 64      # LSTM隐藏层维度（可调）\n        \n        self.lstm = nn.LSTM(\n            input_size=self.input_dim,\n            hidden_size=self.lstm_hidden,\n            batch_first=True\n        )\n        # 将LSTM输出映射到 config.hidden_dim（原解码器要求的维度）\n        self.lstm_proj = nn.Linear(self.lstm_hidden, config.hidden_dim)\n        \n        # ---------- 3. 时序融合解码器（完全复用原代码）----------\n        self.time_decoder = TemporalFusionDecoder(\n            input_dim=config.hidden_dim,\n            hidden_dim=config.hidden_dim,\n            context_dim=config.hidden_dim,\n            output_dim=config.hidden_dim,\n            config=config\n        )\n        \n        # ---------- 4. 输出层：预测单一RASS评分 ----------\n        self.output_layer = nn.Linear(config.hidden_dim, 1)\n        \n    def forward(self, temporal, static):\n        # temporal: (batch, 6, 11)\n        # static:   (batch, 4)\n        \n        # 静态编码\n        static_out, _ = self.static_encoder1(static)  # 原代码返回 (output, weights)\n        cs, ce, ch, cc = self.static_encoder2(static_out)  # cs/ce/ch/cc 都是 (batch, hidden_dim)\n        \n        # 时序编码\n        lstm_out, (hn, cn) = self.lstm(temporal)      # lstm_out: (batch, 6, lstm_hidden)\n        lstm_out = self.lstm_proj(lstm_out)           # (batch, 6, hidden_dim)\n        \n        # 解码器需要 context = ce\n        decoder_out = self.time_decoder(lstm_out, ce)  # (batch, hidden_dim)\n        \n        # 输出预测\n        pred = self.output_layer(decoder_out)          # (batch, 1)\n        return pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:31:35.856173Z","iopub.status.idle":"2026-02-11T14:31:35.856499Z","shell.execute_reply.started":"2026-02-11T14:31:35.856352Z","shell.execute_reply":"2026-02-11T14:31:35.856371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [markdown]\n# # 训练 RASSTransformer - 镇静评分预测\n\n# %% 1. 安装依赖（Kaggle 已预装 PyTorch，无需安装）\nimport sys\nsys.path.append('/kaggle/input/your-code-path')  # 如果你上传了 tranlstm.py，可能需要加路径\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 导入我们刚才写的两个文件\nfrom rass_loader import create_dataloader\nfrom rass_model import RASSTransformer\n\n# %% 2. 定义配置类（原代码使用 config 对象传递参数）\nclass Config:\n    def __init__(self):\n        self.hidden_dim = 64          # 与原代码保持一致，后续可调\n        self.example_length = 6       # 关键：序列长度改为6\n        self.static_dim = 4\n        self.static_num = 4\n        self.temporal_dim = 11\n        self.n_head = 4              # 注意力头数，需 hidden_dim % n_head == 0\n        self.dropout = 0.1\n        self.attn_dropout = 0.1\n\nconfig = Config()\n\n# %% 3. 加载数据\ntrain_loader = create_dataloader(\n    npz_path='/kaggle/input/your-data/training_data_no_bp.npz',  # 请修改为你的文件路径\n    batch_size=32,\n    shuffle=True\n)\n\n# 检查一个 batch\nbatch = next(iter(train_loader))\nprint(f\"temporal shape: {batch['temporal'].shape}\")  # 应为 (32,6,11)\nprint(f\"static shape:   {batch['static'].shape}\")    # 应为 (32,4)\nprint(f\"target shape:   {batch['target'].shape}\")    # 应为 (32,1)\n\n# %% 4. 初始化模型、优化器、损失函数\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel = RASSTransformer(config).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()  # 回归任务用 MSE\n\n# %% 5. 训练循环（最简单版本）\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        x_temporal = batch['temporal'].to(device)\n        x_static   = batch['static'].to(device)\n        y_true     = batch['target'].to(device)\n        \n        optimizer.zero_grad()\n        y_pred = model(x_temporal, x_static)\n        loss = criterion(y_pred, y_true)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * len(y_true)\n    \n    avg_loss = total_loss / len(train_loader.dataset)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n# %% 6. 绘制损失曲线\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\n# %% 7. 保存模型\ntorch.save(model.state_dict(), 'rass_transformer.pth')\nprint(\"Model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:31:35.858468Z","iopub.status.idle":"2026-02-11T14:31:35.858794Z","shell.execute_reply.started":"2026-02-11T14:31:35.858656Z","shell.execute_reply":"2026-02-11T14:31:35.858673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== 1. 检查并确保 tranlstm.py 可用 ====================\nimport sys, os\ntry:\n    from tranlstm import VariableSelectionNetwork, StaticCovariateEncoder, TemporalFusionDecoder, GRN, GLU, InterpretableMultiHeadAttention\n    print(\"✅ tranlstm.py 已就绪\")\nexcept ImportError:\n    # 如果还没有，就写一个最简单的版本（只包含我们需要的类）\n    print(\"⚠️ 正在生成 tranlstm.py...\")\n    %%writefile tranlstm.py\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.nn import LayerNorm\n    from typing import Optional, Tuple\n    import math\n\n    def generate_square_subsequent_mask(sz):\n        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\n    class GLU(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super().__init__()\n            self.fc = nn.Linear(input_dim, output_dim*2)\n        def forward(self, x): return F.glu(self.fc(x))\n\n    class GRN(nn.Module):\n        def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.05, context_dim=None):\n            super().__init__()\n            self.layernorm = nn.LayerNorm(output_dim)\n            self.fc1 = nn.Linear(input_dim, hidden_dim)\n            if context_dim: self.context_fc = nn.Linear(context_dim, hidden_dim)\n            self.skip = nn.Linear(input_dim, output_dim)\n            self.forw = nn.Sequential(\n                nn.ELU(),\n                nn.Linear(hidden_dim, output_dim),\n                nn.Dropout(dropout),\n                GLU(output_dim, output_dim))\n        def forward(self, x, c=None):\n            res = self.skip(x)\n            x = self.fc1(x)\n            if c is not None: x += self.context_fc(c)\n            return self.layernorm(self.forw(x) + res)\n\n    class VariableSelectionNetwork(nn.Module):\n        def __init__(self, input_dim, input_num, hidden_dim, dropout, context_dim=None):\n            super().__init__()\n            self.flattened_grn = GRN(input_num*input_dim, hidden_dim, input_num, dropout, context_dim)\n            self.softmax = nn.Softmax(dim=1)\n            self.single_variable_grns = nn.ModuleList([\n                GRN(input_dim, hidden_dim, hidden_dim, dropout, context_dim) for _ in range(input_num)\n            ])\n        def forward(self, x, context=None):\n            # x: (batch, input_num, input_dim) -> (batch, input_num*input_dim)\n            x_flat = x.reshape(x.shape[0], -1)\n            sparse_weights = self.softmax(self.flattened_grn(x_flat, context)).unsqueeze(2)\n            processed = torch.stack([self.single_variable_grns[i](x[:, i, :], context) for i in range(x.shape[1])], dim=-1)\n            return processed * sparse_weights.transpose(1,2), sparse_weights\n\n    class StaticCovariateEncoder(nn.Module):\n        def __init__(self, hidden_dim):\n            super().__init__()\n            self.vsn = VariableSelectionNetwork(hidden_dim, 4, hidden_dim, 0.1, hidden_dim)\n            self.grn = nn.ModuleList([GRN(hidden_dim, hidden_dim, hidden_dim) for _ in range(4)])\n        def forward(self, x):\n            ctx, _ = self.vsn(x)\n            ctx = ctx.sum(dim=-1)\n            return tuple(m(ctx) for m in self.grn)  # cs, ce, cc, ch\n\n    class InterpretableMultiHeadAttention(nn.Module):\n        def __init__(self, config):\n            super().__init__()\n            self.n_head = config.n_head\n            self.d_head = config.hidden_dim // config.n_head\n            self.qkv_linears = nn.Linear(config.hidden_dim, (2*self.n_head+1)*self.d_head)\n            self.out_proj = nn.Linear(self.d_head, config.hidden_dim)\n            self.attn_dropout = nn.Dropout(config.attn_dropout)\n            self.out_dropout = nn.Dropout(config.dropout)\n            self.scale = self.d_head**-0.5\n            self.register_buffer('_mask', torch.triu(torch.full((config.example_length, config.example_length), float('-inf')),1).unsqueeze(0))\n        def forward(self, x, mask=True):\n            bs, t, _ = x.shape\n            q,k,v = self.qkv_linears(x).split((self.n_head*self.d_head, self.n_head*self.d_head, self.d_head), dim=-1)\n            q = q.view(bs,t,self.n_head,self.d_head).permute(0,2,1,3)\n            k = k.view(bs,t,self.n_head,self.d_head).permute(0,2,3,1)\n            v = v.view(bs,t,self.d_head)\n            attn = torch.matmul(q,k) * self.scale\n            if mask: attn = attn + self._mask\n            attn_prob = self.attn_dropout(F.softmax(attn, dim=-1))\n            attn_vec = torch.matmul(attn_prob, v.unsqueeze(1))\n            out = self.out_proj(attn_vec.mean(dim=1))\n            return self.out_dropout(out), attn_vec\n\n    class TemporalFusionDecoder(nn.Module):\n        def __init__(self, input_dim, hidden_dim, context_dim, output_dim, config):\n            super().__init__()\n            self.example_length = config.example_length\n            self.enrichment_grn = GRN(input_dim, hidden_dim, output_dim, 0.05, context_dim)\n            self.attention = InterpretableMultiHeadAttention(config)\n            self.att_gate = GLU(config.hidden_dim, config.hidden_dim)\n            self.attention_ln = LayerNorm(config.hidden_dim)\n            self.positionwise_grn = GRN(config.hidden_dim, config.hidden_dim, config.hidden_dim, config.dropout)\n            self.decoder_gate = GLU(config.hidden_dim, config.hidden_dim)\n            self.decoder_ln = LayerNorm(config.hidden_dim)\n        def forward(self, x, ce):\n            res = x[:, -1, :]\n            ce = ce.unsqueeze(1).repeat(1, self.example_length, 1)\n            enriched = self.enrichment_grn(x, c=ce)\n            x, _ = self.attention(enriched)\n            x = x[:, -1, :]\n            enriched = enriched[:, -1, :]\n            x = self.att_gate(x) + enriched\n            x = self.attention_ln(x)\n            x = self.positionwise_grn(x)\n            x = self.decoder_gate(x) + res\n            return self.decoder_ln(x)\n    from tranlstm import *\n    print(\"✅ tranlstm.py 已生成并导入\")\n\n# ==================== 2. 数据加载器 ====================\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass RASSDataset(Dataset):\n    def __init__(self, npz_path):\n        data = np.load(npz_path, allow_pickle=True)\n        X = data['X']   # (N, 70)\n        y = data['y']   # (N,)\n        temporal = X[:, :66].reshape(-1, 6, 11)   # (N,6,11)\n        static   = X[:, 66:]                      # (N,4)\n        self.temporal = torch.FloatTensor(temporal)\n        self.static   = torch.FloatTensor(static)\n        self.target   = torch.FloatTensor(y).view(-1, 1)\n\n    def __len__(self): return len(self.target)\n    def __getitem__(self, idx):\n        return {'temporal': self.temporal[idx], 'static': self.static[idx], 'target': self.target[idx]}\n\ndef create_dataloader(npz_path, batch_size=32, shuffle=True):\n    return DataLoader(RASSDataset(npz_path), batch_size=batch_size, shuffle=shuffle)\n\n# ==================== 3. 配置类 ====================\nclass Config:\n    def __init__(self):\n        self.hidden_dim = 64\n        self.example_length = 6      # 关键：序列长度设为6\n        self.static_dim = 4\n        self.static_num = 4\n        self.temporal_dim = 11\n        self.n_head = 4\n        self.dropout = 0.1\n        self.attn_dropout = 0.1\n\n# ==================== 4. 模型定义 ====================\nclass RASSTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.static_encoder1 = VariableSelectionNetwork(\n            config.static_dim, config.static_num, config.hidden_dim, 0.1\n        )\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim)\n        \n        self.lstm = nn.LSTM(input_size=11, hidden_size=64, batch_first=True)\n        self.lstm_proj = nn.Linear(64, config.hidden_dim)\n        \n        self.time_decoder = TemporalFusionDecoder(\n            config.hidden_dim, config.hidden_dim, config.hidden_dim, config.hidden_dim, config\n        )\n        self.output_layer = nn.Linear(config.hidden_dim, 1)\n\n    def forward(self, temporal, static):\n        static_out, _ = self.static_encoder1(static)\n        cs, ce, ch, cc = self.static_encoder2(static_out)\n        \n        lstm_out, _ = self.lstm(temporal)\n        lstm_out = self.lstm_proj(lstm_out)\n        \n        decoder_out = self.time_decoder(lstm_out, ce)\n        return self.output_layer(decoder_out)\n\n# ==================== 5. 训练 ====================\n# ！注意：请修改下面的路径为你的 .npz 文件实际路径\nnpz_path = '/kaggle/input/nbp174/training_data_no_bp.npz'\n\n# 确认路径是否正确\nimport os\nif not os.path.exists(npz_path):\n    print(\"❌ 文件不存在！当前 input 目录下的文件有：\")\n    !ls /kaggle/input/nbp无血压版174个小样本/\n    # 如果文件名不同，请手动修改上面的 npz_path\n    raise FileNotFoundError(\"请修改 npz_path 为正确的文件名\")\n\nconfig = Config()\ntrain_loader = create_dataloader(npz_path, batch_size=32, shuffle=True)\n\n# 检查一个 batch\nbatch = next(iter(train_loader))\nprint(f\"temporal shape: {batch['temporal'].shape}\")\nprint(f\"static shape:   {batch['static'].shape}\")\nprint(f\"target shape:   {batch['target'].shape}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel = RASSTransformer(config).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.MSELoss()\n\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        x_t = batch['temporal'].to(device)\n        x_s = batch['static'].to(device)\n        y   = batch['target'].to(device)\n        \n        optimizer.zero_grad()\n        y_pred = model(x_t, x_s)\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * len(y)\n    \n    avg_loss = total_loss / len(train_loader.dataset)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n# 画图\nimport matplotlib.pyplot as plt\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\n# 保存模型\ntorch.save(model.state_dict(), 'rass_transformer.pth')\nprint(\"✅ 模型已保存为 rass_transformer.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:39:21.295814Z","iopub.execute_input":"2026-02-11T14:39:21.296170Z","iopub.status.idle":"2026-02-11T14:39:21.386537Z","shell.execute_reply.started":"2026-02-11T14:39:21.296140Z","shell.execute_reply":"2026-02-11T14:39:21.385469Z"}},"outputs":[{"name":"stdout","text":"✅ tranlstm.py 已就绪\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2267477262.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;31m# 检查一个 batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2267477262.py\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[0;34m(npz_path, batch_size, shuffle)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRASSDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m# ==================== 3. 配置类 ====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2267477262.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, npz_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mtemporal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m66\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (N,6,11)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mstatic\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m66\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m                      \u001b[0;31m# (N,4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."],"ename":"TypeError","evalue":"can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# ==================== 1. 检查并确保 tranlstm.py 可用 ====================\nimport sys, os\ntry:\n    from tranlstm import VariableSelectionNetwork, StaticCovariateEncoder, TemporalFusionDecoder, GRN, GLU, InterpretableMultiHeadAttention\n    print(\"✅ tranlstm.py 已就绪\")\nexcept ImportError:\n    print(\"⚠️ 正在生成 tranlstm.py...\")\n    %%writefile tranlstm.py\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.nn import LayerNorm\n    from typing import Optional, Tuple\n    import math\n\n    def generate_square_subsequent_mask(sz):\n        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\n    class GLU(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super().__init__()\n            self.fc = nn.Linear(input_dim, output_dim*2)\n        def forward(self, x): return F.glu(self.fc(x))\n\n    class GRN(nn.Module):\n        def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.05, context_dim=None):\n            super().__init__()\n            self.layernorm = nn.LayerNorm(output_dim)\n            self.fc1 = nn.Linear(input_dim, hidden_dim)\n            if context_dim: self.context_fc = nn.Linear(context_dim, hidden_dim)\n            self.skip = nn.Linear(input_dim, output_dim)\n            self.forw = nn.Sequential(\n                nn.ELU(),\n                nn.Linear(hidden_dim, output_dim),\n                nn.Dropout(dropout),\n                GLU(output_dim, output_dim))\n        def forward(self, x, c=None):\n            res = self.skip(x)\n            x = self.fc1(x)\n            if c is not None: x += self.context_fc(c)\n            return self.layernorm(self.forw(x) + res)\n\n    class VariableSelectionNetwork(nn.Module):\n        def __init__(self, input_dim, input_num, hidden_dim, dropout, context_dim=None):\n            super().__init__()\n            self.flattened_grn = GRN(input_num*input_dim, hidden_dim, input_num, dropout, context_dim)\n            self.softmax = nn.Softmax(dim=1)\n            self.single_variable_grns = nn.ModuleList([\n                GRN(input_dim, hidden_dim, hidden_dim, dropout, context_dim) for _ in range(input_num)\n            ])\n        def forward(self, x, context=None):\n            x_flat = x.reshape(x.shape[0], -1)\n            sparse_weights = self.softmax(self.flattened_grn(x_flat, context)).unsqueeze(2)\n            processed = torch.stack([self.single_variable_grns[i](x[:, i, :], context) for i in range(x.shape[1])], dim=-1)\n            return processed * sparse_weights.transpose(1,2), sparse_weights\n\n    class StaticCovariateEncoder(nn.Module):\n        def __init__(self, hidden_dim):\n            super().__init__()\n            self.vsn = VariableSelectionNetwork(hidden_dim, 4, hidden_dim, 0.1, hidden_dim)\n            self.grn = nn.ModuleList([GRN(hidden_dim, hidden_dim, hidden_dim) for _ in range(4)])\n        def forward(self, x):\n            ctx, _ = self.vsn(x)\n            ctx = ctx.sum(dim=-1)\n            return tuple(m(ctx) for m in self.grn)\n\n    class InterpretableMultiHeadAttention(nn.Module):\n        def __init__(self, config):\n            super().__init__()\n            self.n_head = config.n_head\n            self.d_head = config.hidden_dim // config.n_head\n            self.qkv_linears = nn.Linear(config.hidden_dim, (2*self.n_head+1)*self.d_head)\n            self.out_proj = nn.Linear(self.d_head, config.hidden_dim)\n            self.attn_dropout = nn.Dropout(config.attn_dropout)\n            self.out_dropout = nn.Dropout(config.dropout)\n            self.scale = self.d_head**-0.5\n            self.register_buffer('_mask', torch.triu(torch.full((config.example_length, config.example_length), float('-inf')),1).unsqueeze(0))\n        def forward(self, x, mask=True):\n            bs, t, _ = x.shape\n            q,k,v = self.qkv_linears(x).split((self.n_head*self.d_head, self.n_head*self.d_head, self.d_head), dim=-1)\n            q = q.view(bs,t,self.n_head,self.d_head).permute(0,2,1,3)\n            k = k.view(bs,t,self.n_head,self.d_head).permute(0,2,3,1)\n            v = v.view(bs,t,self.d_head)\n            attn = torch.matmul(q,k) * self.scale\n            if mask: attn = attn + self._mask\n            attn_prob = self.attn_dropout(F.softmax(attn, dim=-1))\n            attn_vec = torch.matmul(attn_prob, v.unsqueeze(1))\n            out = self.out_proj(attn_vec.mean(dim=1))\n            return self.out_dropout(out), attn_vec\n\n    class TemporalFusionDecoder(nn.Module):\n        def __init__(self, input_dim, hidden_dim, context_dim, output_dim, config):\n            super().__init__()\n            self.example_length = config.example_length\n            self.enrichment_grn = GRN(input_dim, hidden_dim, output_dim, 0.05, context_dim)\n            self.attention = InterpretableMultiHeadAttention(config)\n            self.att_gate = GLU(config.hidden_dim, config.hidden_dim)\n            self.attention_ln = LayerNorm(config.hidden_dim)\n            self.positionwise_grn = GRN(config.hidden_dim, config.hidden_dim, config.hidden_dim, config.dropout)\n            self.decoder_gate = GLU(config.hidden_dim, config.hidden_dim)\n            self.decoder_ln = LayerNorm(config.hidden_dim)\n        def forward(self, x, ce):\n            res = x[:, -1, :]\n            ce = ce.unsqueeze(1).repeat(1, self.example_length, 1)\n            enriched = self.enrichment_grn(x, c=ce)\n            x, _ = self.attention(enriched)\n            x = x[:, -1, :]\n            enriched = enriched[:, -1, :]\n            x = self.att_gate(x) + enriched\n            x = self.attention_ln(x)\n            x = self.positionwise_grn(x)\n            x = self.decoder_gate(x) + res\n            return self.decoder_ln(x)\n    from tranlstm import *\n    print(\"✅ tranlstm.py 已生成并导入\")\n\n# ==================== 2. 数据加载器（已修复 allow_pickle=True）====================\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass RASSDataset(Dataset):\n    def __init__(self, npz_path):\n        # 关键修复：添加 allow_pickle=True\n        data = np.load(npz_path, allow_pickle=True)\n        X = data['X']   # (N, 70)\n        y = data['y']   # (N,)\n        temporal = X[:, :66].reshape(-1, 6, 11)   # (N,6,11)\n        static   = X[:, 66:]                      # (N,4)\n        self.temporal = torch.FloatTensor(temporal)\n        self.static   = torch.FloatTensor(static)\n        self.target   = torch.FloatTensor(y).view(-1, 1)\n\n    def __len__(self): return len(self.target)\n    def __getitem__(self, idx):\n        return {'temporal': self.temporal[idx], 'static': self.static[idx], 'target': self.target[idx]}\n\ndef create_dataloader(npz_path, batch_size=32, shuffle=True):\n    return DataLoader(RASSDataset(npz_path), batch_size=batch_size, shuffle=shuffle)\n\n# ==================== 3. 配置类 ====================\nclass Config:\n    def __init__(self):\n        self.hidden_dim = 64\n        self.example_length = 6      # 关键：序列长度设为6\n        self.static_dim = 4\n        self.static_num = 4\n        self.temporal_dim = 11\n        self.n_head = 4\n        self.dropout = 0.1\n        self.attn_dropout = 0.1\n\n# ==================== 4. 模型定义 ====================\nclass RASSTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.static_encoder1 = VariableSelectionNetwork(\n            config.static_dim, config.static_num, config.hidden_dim, 0.1\n        )\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim)\n        \n        self.lstm = nn.LSTM(input_size=11, hidden_size=64, batch_first=True)\n        self.lstm_proj = nn.Linear(64, config.hidden_dim)\n        \n        self.time_decoder = TemporalFusionDecoder(\n            config.hidden_dim, config.hidden_dim, config.hidden_dim, config.hidden_dim, config\n        )\n        self.output_layer = nn.Linear(config.hidden_dim, 1)\n\n    def forward(self, temporal, static):\n        static_out, _ = self.static_encoder1(static)\n        cs, ce, ch, cc = self.static_encoder2(static_out)\n        \n        lstm_out, _ = self.lstm(temporal)\n        lstm_out = self.lstm_proj(lstm_out)\n        \n        decoder_out = self.time_decoder(lstm_out, ce)\n        return self.output_layer(decoder_out)\n\n# ==================== 5. 训练 ====================\n# ！请确认路径正确（你已找到的路径）\nnpz_path = '/kaggle/input/nbp174/training_data_no_bp.npz'\n\n# 检查文件是否存在\nimport os\nif not os.path.exists(npz_path):\n    print(\"❌ 文件不存在！请检查路径。\")\n    print(\"当前 input 目录下的文件有：\")\n    !ls /kaggle/input/nbp174/\n    raise FileNotFoundError(\"请修改 npz_path 为正确的路径\")\nelse:\n    print(\"✅ 文件存在，开始加载数据...\")\n\nconfig = Config()\ntrain_loader = create_dataloader(npz_path, batch_size=32, shuffle=True)\n\n# 检查一个 batch\nbatch = next(iter(train_loader))\nprint(f\"temporal shape: {batch['temporal'].shape}\")\nprint(f\"static shape:   {batch['static'].shape}\")\nprint(f\"target shape:   {batch['target'].shape}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel = RASSTransformer(config).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.MSELoss()\n\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        x_t = batch['temporal'].to(device)\n        x_s = batch['static'].to(device)\n        y   = batch['target'].to(device)\n        \n        optimizer.zero_grad()\n        y_pred = model(x_t, x_s)\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * len(y)\n    \n    avg_loss = total_loss / len(train_loader.dataset)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n# 画图\nimport matplotlib.pyplot as plt\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\n# 保存模型\ntorch.save(model.state_dict(), 'rass_transformer.pth')\nprint(\"✅ 模型已保存为 rass_transformer.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:42:08.353723Z","iopub.execute_input":"2026-02-11T14:42:08.354455Z","iopub.status.idle":"2026-02-11T14:42:08.431640Z","shell.execute_reply.started":"2026-02-11T14:42:08.354421Z","shell.execute_reply":"2026-02-11T14:42:08.430165Z"}},"outputs":[{"name":"stdout","text":"✅ tranlstm.py 已就绪\n✅ 文件存在，开始加载数据...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3999697357.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;31m# 检查一个 batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3999697357.py\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[0;34m(npz_path, batch_size, shuffle)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRASSDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m# ==================== 3. 配置类 ====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3999697357.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, npz_path)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mtemporal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m66\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (N,6,11)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mstatic\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m66\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m                      \u001b[0;31m# (N,4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."],"ename":"TypeError","evalue":"can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"# ==================== 1. 检查并确保 tranlstm.py 可用 ====================\nimport sys, os\ntry:\n    from tranlstm import VariableSelectionNetwork, StaticCovariateEncoder, TemporalFusionDecoder, GRN, GLU, InterpretableMultiHeadAttention\n    print(\"✅ tranlstm.py 已就绪\")\nexcept ImportError:\n    print(\"⚠️ 正在生成 tranlstm.py...\")\n    %%writefile tranlstm.py\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.nn import LayerNorm\n    from typing import Optional, Tuple\n    import math\n\n    def generate_square_subsequent_mask(sz):\n        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\n    class GLU(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super().__init__()\n            self.fc = nn.Linear(input_dim, output_dim*2)\n        def forward(self, x): return F.glu(self.fc(x))\n\n    class GRN(nn.Module):\n        def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.05, context_dim=None):\n            super().__init__()\n            self.layernorm = nn.LayerNorm(output_dim)\n            self.fc1 = nn.Linear(input_dim, hidden_dim)\n            if context_dim: self.context_fc = nn.Linear(context_dim, hidden_dim)\n            self.skip = nn.Linear(input_dim, output_dim)\n            self.forw = nn.Sequential(\n                nn.ELU(),\n                nn.Linear(hidden_dim, output_dim),\n                nn.Dropout(dropout),\n                GLU(output_dim, output_dim))\n        def forward(self, x, c=None):\n            res = self.skip(x)\n            x = self.fc1(x)\n            if c is not None: x += self.context_fc(c)\n            return self.layernorm(self.forw(x) + res)\n\n    class VariableSelectionNetwork(nn.Module):\n        def __init__(self, input_dim, input_num, hidden_dim, dropout, context_dim=None):\n            super().__init__()\n            self.flattened_grn = GRN(input_num*input_dim, hidden_dim, input_num, dropout, context_dim)\n            self.softmax = nn.Softmax(dim=1)\n            self.single_variable_grns = nn.ModuleList([\n                GRN(input_dim, hidden_dim, hidden_dim, dropout, context_dim) for _ in range(input_num)\n            ])\n        def forward(self, x, context=None):\n            x_flat = x.reshape(x.shape[0], -1)\n            sparse_weights = self.softmax(self.flattened_grn(x_flat, context)).unsqueeze(2)\n            processed = torch.stack([self.single_variable_grns[i](x[:, i, :], context) for i in range(x.shape[1])], dim=-1)\n            return processed * sparse_weights.transpose(1,2), sparse_weights\n\n    class StaticCovariateEncoder(nn.Module):\n        def __init__(self, hidden_dim):\n            super().__init__()\n            self.vsn = VariableSelectionNetwork(hidden_dim, 4, hidden_dim, 0.1, hidden_dim)\n            self.grn = nn.ModuleList([GRN(hidden_dim, hidden_dim, hidden_dim) for _ in range(4)])\n        def forward(self, x):\n            ctx, _ = self.vsn(x)\n            ctx = ctx.sum(dim=-1)\n            return tuple(m(ctx) for m in self.grn)\n\n    class InterpretableMultiHeadAttention(nn.Module):\n        def __init__(self, config):\n            super().__init__()\n            self.n_head = config.n_head\n            self.d_head = config.hidden_dim // config.n_head\n            self.qkv_linears = nn.Linear(config.hidden_dim, (2*self.n_head+1)*self.d_head)\n            self.out_proj = nn.Linear(self.d_head, config.hidden_dim)\n            self.attn_dropout = nn.Dropout(config.attn_dropout)\n            self.out_dropout = nn.Dropout(config.dropout)\n            self.scale = self.d_head**-0.5\n            self.register_buffer('_mask', torch.triu(torch.full((config.example_length, config.example_length), float('-inf')),1).unsqueeze(0))\n        def forward(self, x, mask=True):\n            bs, t, _ = x.shape\n            q,k,v = self.qkv_linears(x).split((self.n_head*self.d_head, self.n_head*self.d_head, self.d_head), dim=-1)\n            q = q.view(bs,t,self.n_head,self.d_head).permute(0,2,1,3)\n            k = k.view(bs,t,self.n_head,self.d_head).permute(0,2,3,1)\n            v = v.view(bs,t,self.d_head)\n            attn = torch.matmul(q,k) * self.scale\n            if mask: attn = attn + self._mask\n            attn_prob = self.attn_dropout(F.softmax(attn, dim=-1))\n            attn_vec = torch.matmul(attn_prob, v.unsqueeze(1))\n            out = self.out_proj(attn_vec.mean(dim=1))\n            return self.out_dropout(out), attn_vec\n\n    class TemporalFusionDecoder(nn.Module):\n        def __init__(self, input_dim, hidden_dim, context_dim, output_dim, config):\n            super().__init__()\n            self.example_length = config.example_length\n            self.enrichment_grn = GRN(input_dim, hidden_dim, output_dim, 0.05, context_dim)\n            self.attention = InterpretableMultiHeadAttention(config)\n            self.att_gate = GLU(config.hidden_dim, config.hidden_dim)\n            self.attention_ln = LayerNorm(config.hidden_dim)\n            self.positionwise_grn = GRN(config.hidden_dim, config.hidden_dim, config.hidden_dim, config.dropout)\n            self.decoder_gate = GLU(config.hidden_dim, config.hidden_dim)\n            self.decoder_ln = LayerNorm(config.hidden_dim)\n        def forward(self, x, ce):\n            res = x[:, -1, :]\n            ce = ce.unsqueeze(1).repeat(1, self.example_length, 1)\n            enriched = self.enrichment_grn(x, c=ce)\n            x, _ = self.attention(enriched)\n            x = x[:, -1, :]\n            enriched = enriched[:, -1, :]\n            x = self.att_gate(x) + enriched\n            x = self.attention_ln(x)\n            x = self.positionwise_grn(x)\n            x = self.decoder_gate(x) + res\n            return self.decoder_ln(x)\n    from tranlstm import *\n    print(\"✅ tranlstm.py 已生成并导入\")\n\n# ==================== 2. 数据加载器（强制转换为 float32）====================\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass RASSDataset(Dataset):\n    def __init__(self, npz_path):\n        # 加载时允许 pickle 对象\n        data = np.load(npz_path, allow_pickle=True)\n        X = data['X']   # (N, 70)\n        y = data['y']   # (N,)\n        \n        # 打印数据类型信息，用于调试\n        print(f\"X shape: {X.shape}, X dtype: {X.dtype}\")\n        print(f\"y shape: {y.shape}, y dtype: {y.dtype}\")\n        \n        # 关键修复：强制转换为 float32，如果包含字符串等无法转换的值，会报错，但先试试\n        try:\n            X = X.astype(np.float32)\n            y = y.astype(np.float32)\n        except ValueError as e:\n            print(\"转换失败，请检查数据是否包含非数值内容\")\n            raise e\n        \n        temporal = X[:, :66].reshape(-1, 6, 11)   # (N,6,11)\n        static   = X[:, 66:]                      # (N,4)\n        \n        self.temporal = torch.from_numpy(temporal).float()\n        self.static   = torch.from_numpy(static).float()\n        self.target   = torch.from_numpy(y).float().view(-1, 1)\n\n    def __len__(self): \n        return len(self.target)\n    \n    def __getitem__(self, idx):\n        return {\n            'temporal': self.temporal[idx],\n            'static':   self.static[idx],\n            'target':   self.target[idx]\n        }\n\ndef create_dataloader(npz_path, batch_size=32, shuffle=True):\n    return DataLoader(RASSDataset(npz_path), batch_size=batch_size, shuffle=shuffle)\n\n# ==================== 3. 配置类 ====================\nclass Config:\n    def __init__(self):\n        self.hidden_dim = 64\n        self.example_length = 6      # 关键：序列长度设为6\n        self.static_dim = 4\n        self.static_num = 4\n        self.temporal_dim = 11\n        self.n_head = 4\n        self.dropout = 0.1\n        self.attn_dropout = 0.1\n\n# ==================== 4. 模型定义 ====================\nclass RASSTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.static_encoder1 = VariableSelectionNetwork(\n            config.static_dim, config.static_num, config.hidden_dim, 0.1\n        )\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim)\n        \n        self.lstm = nn.LSTM(input_size=11, hidden_size=64, batch_first=True)\n        self.lstm_proj = nn.Linear(64, config.hidden_dim)\n        \n        self.time_decoder = TemporalFusionDecoder(\n            config.hidden_dim, config.hidden_dim, config.hidden_dim, config.hidden_dim, config\n        )\n        self.output_layer = nn.Linear(config.hidden_dim, 1)\n\n    def forward(self, temporal, static):\n        static_out, _ = self.static_encoder1(static)\n        cs, ce, ch, cc = self.static_encoder2(static_out)\n        \n        lstm_out, _ = self.lstm(temporal)\n        lstm_out = self.lstm_proj(lstm_out)\n        \n        decoder_out = self.time_decoder(lstm_out, ce)\n        return self.output_layer(decoder_out)\n\n# ==================== 5. 训练 ====================\n# 你的文件路径\nnpz_path = '/kaggle/input/nbp174/training_data_no_bp.npz'\n\n# 检查文件是否存在\nimport os\nif not os.path.exists(npz_path):\n    print(\"❌ 文件不存在！请检查路径。\")\n    print(\"当前 input 目录下的文件有：\")\n    !ls /kaggle/input/nbp174/\n    raise FileNotFoundError(\"请修改 npz_path 为正确的路径\")\nelse:\n    print(\"✅ 文件存在，开始加载数据...\")\n\nconfig = Config()\ntrain_loader = create_dataloader(npz_path, batch_size=32, shuffle=True)\n\n# 检查一个 batch\nbatch = next(iter(train_loader))\nprint(f\"temporal shape: {batch['temporal'].shape}\")\nprint(f\"static shape:   {batch['static'].shape}\")\nprint(f\"target shape:   {batch['target'].shape}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel = RASSTransformer(config).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.MSELoss()\n\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        x_t = batch['temporal'].to(device)\n        x_s = batch['static'].to(device)\n        y   = batch['target'].to(device)\n        \n        optimizer.zero_grad()\n        y_pred = model(x_t, x_s)\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * len(y)\n    \n    avg_loss = total_loss / len(train_loader.dataset)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n# 画图\nimport matplotlib.pyplot as plt\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\n# 保存模型\ntorch.save(model.state_dict(), 'rass_transformer.pth')\nprint(\"✅ 模型已保存为 rass_transformer.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:43:38.102007Z","iopub.execute_input":"2026-02-11T14:43:38.102361Z","iopub.status.idle":"2026-02-11T14:43:38.274256Z","shell.execute_reply.started":"2026-02-11T14:43:38.102332Z","shell.execute_reply":"2026-02-11T14:43:38.273189Z"}},"outputs":[{"name":"stdout","text":"✅ tranlstm.py 已就绪\n✅ 文件存在，开始加载数据...\nX shape: (174, 70), X dtype: object\ny shape: (174,), y dtype: float64\ntemporal shape: torch.Size([32, 6, 11])\nstatic shape:   torch.Size([32, 4])\ntarget shape:   torch.Size([32, 1])\nUsing device: cpu\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3834789508.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using device: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRASSTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3834789508.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         )\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_encoder2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStaticCovariateEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: StaticCovariateEncoder.__init__() missing 1 required positional argument: 'hidden_dim'"],"ename":"TypeError","evalue":"StaticCovariateEncoder.__init__() missing 1 required positional argument: 'hidden_dim'","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"# ==================== 1. 生成完整的 tranlstm.py（所有类定义正确）====================\nimport sys, os\n\n%%writefile tranlstm.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\nfrom typing import Optional, Tuple\nimport math\n\ndef generate_square_subsequent_mask(sz):\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\nclass GLU(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, output_dim*2)\n    def forward(self, x): return F.glu(self.fc(x))\n\nclass GRN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.05, context_dim=None):\n        super().__init__()\n        self.layernorm = nn.LayerNorm(output_dim)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        if context_dim: self.context_fc = nn.Linear(context_dim, hidden_dim)\n        self.skip = nn.Linear(input_dim, output_dim)\n        self.forw = nn.Sequential(\n            nn.ELU(),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Dropout(dropout),\n            GLU(output_dim, output_dim))\n    def forward(self, x, c=None):\n        res = self.skip(x)\n        x = self.fc1(x)\n        if c is not None: x += self.context_fc(c)\n        return self.layernorm(self.forw(x) + res)\n\nclass VariableSelectionNetwork(nn.Module):\n    def __init__(self, input_dim, input_num, hidden_dim, dropout, context_dim=None):\n        super().__init__()\n        self.flattened_grn = GRN(input_num*input_dim, hidden_dim, input_num, dropout, context_dim)\n        self.softmax = nn.Softmax(dim=1)\n        self.single_variable_grns = nn.ModuleList([\n            GRN(input_dim, hidden_dim, hidden_dim, dropout, context_dim) for _ in range(input_num)\n        ])\n    def forward(self, x, context=None):\n        x_flat = x.reshape(x.shape[0], -1)\n        sparse_weights = self.softmax(self.flattened_grn(x_flat, context)).unsqueeze(2)\n        processed = torch.stack([self.single_variable_grns[i](x[:, i, :], context) for i in range(x.shape[1])], dim=-1)\n        return processed * sparse_weights.transpose(1,2), sparse_weights\n\n# ************ 关键修复：StaticCovariateEncoder 接受两个参数 ************\nclass StaticCovariateEncoder(nn.Module):\n    def __init__(self, f_dim, hidden_dim):   # 现在接受两个参数\n        super().__init__()\n        self.vsn = VariableSelectionNetwork(hidden_dim, 4, hidden_dim, 0.1, hidden_dim)\n        self.grn = nn.ModuleList([GRN(hidden_dim, hidden_dim, hidden_dim) for _ in range(4)])\n    def forward(self, x):\n        ctx, _ = self.vsn(x)\n        ctx = ctx.sum(dim=-1)\n        return tuple(m(ctx) for m in self.grn)  # cs, ce, cc, ch\n\nclass InterpretableMultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.d_head = config.hidden_dim // config.n_head\n        self.qkv_linears = nn.Linear(config.hidden_dim, (2*self.n_head+1)*self.d_head)\n        self.out_proj = nn.Linear(self.d_head, config.hidden_dim)\n        self.attn_dropout = nn.Dropout(config.attn_dropout)\n        self.out_dropout = nn.Dropout(config.dropout)\n        self.scale = self.d_head**-0.5\n        self.register_buffer('_mask', torch.triu(torch.full((config.example_length, config.example_length), float('-inf')),1).unsqueeze(0))\n    def forward(self, x, mask=True):\n        bs, t, _ = x.shape\n        q,k,v = self.qkv_linears(x).split((self.n_head*self.d_head, self.n_head*self.d_head, self.d_head), dim=-1)\n        q = q.view(bs,t,self.n_head,self.d_head).permute(0,2,1,3)\n        k = k.view(bs,t,self.n_head,self.d_head).permute(0,2,3,1)\n        v = v.view(bs,t,self.d_head)\n        attn = torch.matmul(q,k) * self.scale\n        if mask: attn = attn + self._mask\n        attn_prob = self.attn_dropout(F.softmax(attn, dim=-1))\n        attn_vec = torch.matmul(attn_prob, v.unsqueeze(1))\n        out = self.out_proj(attn_vec.mean(dim=1))\n        return self.out_dropout(out), attn_vec\n\nclass TemporalFusionDecoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, context_dim, output_dim, config):\n        super().__init__()\n        self.example_length = config.example_length\n        self.enrichment_grn = GRN(input_dim, hidden_dim, output_dim, 0.05, context_dim)\n        self.attention = InterpretableMultiHeadAttention(config)\n        self.att_gate = GLU(config.hidden_dim, config.hidden_dim)\n        self.attention_ln = LayerNorm(config.hidden_dim)\n        self.positionwise_grn = GRN(config.hidden_dim, config.hidden_dim, config.hidden_dim, config.dropout)\n        self.decoder_gate = GLU(config.hidden_dim, config.hidden_dim)\n        self.decoder_ln = LayerNorm(config.hidden_dim)\n    def forward(self, x, ce):\n        res = x[:, -1, :]\n        ce = ce.unsqueeze(1).repeat(1, self.example_length, 1)\n        enriched = self.enrichment_grn(x, c=ce)\n        x, _ = self.attention(enriched)\n        x = x[:, -1, :]\n        enriched = enriched[:, -1, :]\n        x = self.att_gate(x) + enriched\n        x = self.attention_ln(x)\n        x = self.positionwise_grn(x)\n        x = self.decoder_gate(x) + res\n        return self.decoder_ln(x)\n\nprint(\"✅ tranlstm.py 已正确生成（StaticCovariateEncoder 已修复）\")\n\n# ==================== 2. 导入 ====================\nfrom tranlstm import *\n\n# ==================== 3. 数据加载器（强制 float32）====================\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass RASSDataset(Dataset):\n    def __init__(self, npz_path):\n        data = np.load(npz_path, allow_pickle=True)\n        X = data['X']\n        y = data['y']\n        print(f\"X shape: {X.shape}, X dtype: {X.dtype}\")\n        print(f\"y shape: {y.shape}, y dtype: {y.dtype}\")\n        # 强制转换为 float32\n        X = X.astype(np.float32)\n        y = y.astype(np.float32)\n        temporal = X[:, :66].reshape(-1, 6, 11)\n        static   = X[:, 66:]\n        self.temporal = torch.from_numpy(temporal).float()\n        self.static   = torch.from_numpy(static).float()\n        self.target   = torch.from_numpy(y).float().view(-1, 1)\n    def __len__(self): return len(self.target)\n    def __getitem__(self, idx):\n        return {'temporal': self.temporal[idx], 'static': self.static[idx], 'target': self.target[idx]}\n\ndef create_dataloader(npz_path, batch_size=32, shuffle=True):\n    return DataLoader(RASSDataset(npz_path), batch_size=batch_size, shuffle=shuffle)\n\n# ==================== 4. 配置类 ====================\nclass Config:\n    def __init__(self):\n        self.hidden_dim = 64\n        self.example_length = 6\n        self.static_dim = 4\n        self.static_num = 4\n        self.temporal_dim = 11\n        self.n_head = 4\n        self.dropout = 0.1\n        self.attn_dropout = 0.1\n\n# ==================== 5. 模型定义 ====================\nclass RASSTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.static_encoder1 = VariableSelectionNetwork(\n            config.static_dim, config.static_num, config.hidden_dim, 0.1\n        )\n        # 关键：现在 StaticCovariateEncoder 接受两个参数，符合原代码\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim, config.hidden_dim)\n        \n        self.lstm = nn.LSTM(input_size=11, hidden_size=64, batch_first=True)\n        self.lstm_proj = nn.Linear(64, config.hidden_dim)\n        \n        self.time_decoder = TemporalFusionDecoder(\n            config.hidden_dim, config.hidden_dim, config.hidden_dim, config.hidden_dim, config\n        )\n        self.output_layer = nn.Linear(config.hidden_dim, 1)\n\n    def forward(self, temporal, static):\n        static_out, _ = self.static_encoder1(static)\n        cs, ce, ch, cc = self.static_encoder2(static_out)\n        lstm_out, _ = self.lstm(temporal)\n        lstm_out = self.lstm_proj(lstm_out)\n        decoder_out = self.time_decoder(lstm_out, ce)\n        return self.output_layer(decoder_out)\n\n# ==================== 6. 训练 ====================\nnpz_path = '/kaggle/input/nbp174/training_data_no_bp.npz'  # 请确保路径正确\n\nimport os\nif not os.path.exists(npz_path):\n    print(\"❌ 文件不存在！请检查路径。\")\n    !ls /kaggle/input/nbp174/\n    raise FileNotFoundError(\"请修改 npz_path 为正确的路径\")\nelse:\n    print(\"✅ 文件存在，开始加载数据...\")\n\nconfig = Config()\ntrain_loader = create_dataloader(npz_path, batch_size=32, shuffle=True)\n\n# 检查一个 batch\nbatch = next(iter(train_loader))\nprint(f\"temporal shape: {batch['temporal'].shape}\")\nprint(f\"static shape:   {batch['static'].shape}\")\nprint(f\"target shape:   {batch['target'].shape}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel = RASSTransformer(config).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.MSELoss()\n\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        x_t = batch['temporal'].to(device)\n        x_s = batch['static'].to(device)\n        y   = batch['target'].to(device)\n        \n        optimizer.zero_grad()\n        y_pred = model(x_t, x_s)\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * len(y)\n    \n    avg_loss = total_loss / len(train_loader.dataset)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\nimport matplotlib.pyplot as plt\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\ntorch.save(model.state_dict(), 'rass_transformer.pth')\nprint(\"✅ 模型已保存为 rass_transformer.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:45:31.255445Z","iopub.execute_input":"2026-02-11T14:45:31.255847Z","iopub.status.idle":"2026-02-11T14:45:31.322981Z","shell.execute_reply.started":"2026-02-11T14:45:31.255813Z","shell.execute_reply":"2026-02-11T14:45:31.321780Z"}},"outputs":[{"name":"stderr","text":"UsageError: Line magic function `%%writefile` not found.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ==================== 1. 生成完整的 tranlstm.py（用 Python 文件写入）====================\nimport os\n\ntranlstm_code = '''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\nfrom typing import Optional, Tuple\nimport math\n\ndef generate_square_subsequent_mask(sz):\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\nclass GLU(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, output_dim*2)\n    def forward(self, x): return F.glu(self.fc(x))\n\nclass GRN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.05, context_dim=None):\n        super().__init__()\n        self.layernorm = nn.LayerNorm(output_dim)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        if context_dim: self.context_fc = nn.Linear(context_dim, hidden_dim)\n        self.skip = nn.Linear(input_dim, output_dim)\n        self.forw = nn.Sequential(\n            nn.ELU(),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Dropout(dropout),\n            GLU(output_dim, output_dim))\n    def forward(self, x, c=None):\n        res = self.skip(x)\n        x = self.fc1(x)\n        if c is not None: x += self.context_fc(c)\n        return self.layernorm(self.forw(x) + res)\n\nclass VariableSelectionNetwork(nn.Module):\n    def __init__(self, input_dim, input_num, hidden_dim, dropout, context_dim=None):\n        super().__init__()\n        self.flattened_grn = GRN(input_num*input_dim, hidden_dim, input_num, dropout, context_dim)\n        self.softmax = nn.Softmax(dim=1)\n        self.single_variable_grns = nn.ModuleList([\n            GRN(input_dim, hidden_dim, hidden_dim, dropout, context_dim) for _ in range(input_num)\n        ])\n    def forward(self, x, context=None):\n        x_flat = x.reshape(x.shape[0], -1)\n        sparse_weights = self.softmax(self.flattened_grn(x_flat, context)).unsqueeze(2)\n        processed = torch.stack([self.single_variable_grns[i](x[:, i, :], context) for i in range(x.shape[1])], dim=-1)\n        return processed * sparse_weights.transpose(1,2), sparse_weights\n\nclass StaticCovariateEncoder(nn.Module):\n    def __init__(self, f_dim, hidden_dim):\n        super().__init__()\n        self.vsn = VariableSelectionNetwork(hidden_dim, 4, hidden_dim, 0.1, hidden_dim)\n        self.grn = nn.ModuleList([GRN(hidden_dim, hidden_dim, hidden_dim) for _ in range(4)])\n    def forward(self, x):\n        ctx, _ = self.vsn(x)\n        ctx = ctx.sum(dim=-1)\n        return tuple(m(ctx) for m in self.grn)\n\nclass InterpretableMultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.d_head = config.hidden_dim // config.n_head\n        self.qkv_linears = nn.Linear(config.hidden_dim, (2*self.n_head+1)*self.d_head)\n        self.out_proj = nn.Linear(self.d_head, config.hidden_dim)\n        self.attn_dropout = nn.Dropout(config.attn_dropout)\n        self.out_dropout = nn.Dropout(config.dropout)\n        self.scale = self.d_head**-0.5\n        self.register_buffer('_mask', torch.triu(torch.full((config.example_length, config.example_length), float('-inf')),1).unsqueeze(0))\n    def forward(self, x, mask=True):\n        bs, t, _ = x.shape\n        q,k,v = self.qkv_linears(x).split((self.n_head*self.d_head, self.n_head*self.d_head, self.d_head), dim=-1)\n        q = q.view(bs,t,self.n_head,self.d_head).permute(0,2,1,3)\n        k = k.view(bs,t,self.n_head,self.d_head).permute(0,2,3,1)\n        v = v.view(bs,t,self.d_head)\n        attn = torch.matmul(q,k) * self.scale\n        if mask: attn = attn + self._mask\n        attn_prob = self.attn_dropout(F.softmax(attn, dim=-1))\n        attn_vec = torch.matmul(attn_prob, v.unsqueeze(1))\n        out = self.out_proj(attn_vec.mean(dim=1))\n        return self.out_dropout(out), attn_vec\n\nclass TemporalFusionDecoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, context_dim, output_dim, config):\n        super().__init__()\n        self.example_length = config.example_length\n        self.enrichment_grn = GRN(input_dim, hidden_dim, output_dim, 0.05, context_dim)\n        self.attention = InterpretableMultiHeadAttention(config)\n        self.att_gate = GLU(config.hidden_dim, config.hidden_dim)\n        self.attention_ln = LayerNorm(config.hidden_dim)\n        self.positionwise_grn = GRN(config.hidden_dim, config.hidden_dim, config.hidden_dim, config.dropout)\n        self.decoder_gate = GLU(config.hidden_dim, config.hidden_dim)\n        self.decoder_ln = LayerNorm(config.hidden_dim)\n    def forward(self, x, ce):\n        res = x[:, -1, :]\n        ce = ce.unsqueeze(1).repeat(1, self.example_length, 1)\n        enriched = self.enrichment_grn(x, c=ce)\n        x, _ = self.attention(enriched)\n        x = x[:, -1, :]\n        enriched = enriched[:, -1, :]\n        x = self.att_gate(x) + enriched\n        x = self.attention_ln(x)\n        x = self.positionwise_grn(x)\n        x = self.decoder_gate(x) + res\n        return self.decoder_ln(x)\n'''\n\n# 写入 tranlstm.py\nwith open('tranlstm.py', 'w') as f:\n    f.write(tranlstm_code)\nprint(\"✅ tranlstm.py 已生成\")\n\n# ==================== 2. 导入模块 ====================\nfrom tranlstm import *\n\n# ==================== 3. 数据加载器 ====================\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass RASSDataset(Dataset):\n    def __init__(self, npz_path):\n        data = np.load(npz_path, allow_pickle=True)\n        X = data['X']\n        y = data['y']\n        print(f\"X shape: {X.shape}, X dtype: {X.dtype}\")\n        print(f\"y shape: {y.shape}, y dtype: {y.dtype}\")\n        # 强制转换为 float32\n        X = X.astype(np.float32)\n        y = y.astype(np.float32)\n        temporal = X[:, :66].reshape(-1, 6, 11)\n        static   = X[:, 66:]\n        self.temporal = torch.from_numpy(temporal).float()\n        self.static   = torch.from_numpy(static).float()\n        self.target   = torch.from_numpy(y).float().view(-1, 1)\n\n    def __len__(self): \n        return len(self.target)\n    \n    def __getitem__(self, idx):\n        return {\n            'temporal': self.temporal[idx],\n            'static':   self.static[idx],\n            'target':   self.target[idx]\n        }\n\ndef create_dataloader(npz_path, batch_size=32, shuffle=True):\n    return DataLoader(RASSDataset(npz_path), batch_size=batch_size, shuffle=shuffle)\n\n# ==================== 4. 配置类 ====================\nclass Config:\n    def __init__(self):\n        self.hidden_dim = 64\n        self.example_length = 6\n        self.static_dim = 4\n        self.static_num = 4\n        self.temporal_dim = 11\n        self.n_head = 4\n        self.dropout = 0.1\n        self.attn_dropout = 0.1\n\n# ==================== 5. 模型定义 ====================\nclass RASSTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.static_encoder1 = VariableSelectionNetwork(\n            config.static_dim, config.static_num, config.hidden_dim, 0.1\n        )\n        self.static_encoder2 = StaticCovariateEncoder(config.hidden_dim, config.hidden_dim)\n        \n        self.lstm = nn.LSTM(input_size=11, hidden_size=64, batch_first=True)\n        self.lstm_proj = nn.Linear(64, config.hidden_dim)\n        \n        self.time_decoder = TemporalFusionDecoder(\n            config.hidden_dim, config.hidden_dim, config.hidden_dim, config.hidden_dim, config\n        )\n        self.output_layer = nn.Linear(config.hidden_dim, 1)\n\n    def forward(self, temporal, static):\n        static_out, _ = self.static_encoder1(static)\n        cs, ce, ch, cc = self.static_encoder2(static_out)\n        \n        lstm_out, _ = self.lstm(temporal)\n        lstm_out = self.lstm_proj(lstm_out)\n        \n        decoder_out = self.time_decoder(lstm_out, ce)\n        return self.output_layer(decoder_out)\n\n# ==================== 6. 训练 ====================\nnpz_path = '/kaggle/input/nbp174/training_data_no_bp.npz'  # 请根据实际情况修改\n\nimport os\nif not os.path.exists(npz_path):\n    print(\"❌ 文件不存在！请检查路径。\")\n    !ls /kaggle/input/nbp174/\n    raise FileNotFoundError(\"请修改 npz_path 为正确的路径\")\nelse:\n    print(\"✅ 文件存在，开始加载数据...\")\n\nconfig = Config()\ntrain_loader = create_dataloader(npz_path, batch_size=32, shuffle=True)\n\n# 检查一个 batch\nbatch = next(iter(train_loader))\nprint(f\"temporal shape: {batch['temporal'].shape}\")\nprint(f\"static shape:   {batch['static'].shape}\")\nprint(f\"target shape:   {batch['target'].shape}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel = RASSTransformer(config).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.MSELoss()\n\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        x_t = batch['temporal'].to(device)\n        x_s = batch['static'].to(device)\n        y   = batch['target'].to(device)\n        \n        optimizer.zero_grad()\n        y_pred = model(x_t, x_s)\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * len(y)\n    \n    avg_loss = total_loss / len(train_loader.dataset)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n# 画图\nimport matplotlib.pyplot as plt\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\n# 保存模型\ntorch.save(model.state_dict(), 'rass_transformer.pth')\nprint(\"✅ 模型已保存为 rass_transformer.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:47:36.901305Z","iopub.execute_input":"2026-02-11T14:47:36.902327Z","iopub.status.idle":"2026-02-11T14:47:40.371422Z","shell.execute_reply.started":"2026-02-11T14:47:36.902271Z","shell.execute_reply":"2026-02-11T14:47:40.370105Z"}},"outputs":[{"name":"stdout","text":"✅ tranlstm.py 已生成\n✅ 文件存在，开始加载数据...\nX shape: (174, 70), X dtype: object\ny shape: (174,), y dtype: float64\ntemporal shape: torch.Size([32, 6, 11])\nstatic shape:   torch.Size([32, 4])\ntarget shape:   torch.Size([32, 1])\nUsing device: cpu\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1078404041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/1078404041.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, temporal, static)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemporal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mstatic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_encoder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_encoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/tranlstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, flattened_embedding, context)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/tranlstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x4 and 16x4)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (32x4 and 16x4)","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"# ==================== 终极简化 LSTM 模型 ====================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# ---------- 1. 数据加载（已处理所有类型问题）----------\nclass RASSDataset(Dataset):\n    def __init__(self, npz_path):\n        data = np.load(npz_path, allow_pickle=True)\n        X = data['X'].astype(np.float32)   # 强制 float32\n        y = data['y'].astype(np.float32)\n        # 前66维 → (N,6,11)，后4维 → (N,4)\n        self.temporal = torch.from_numpy(X[:, :66].reshape(-1, 6, 11))\n        self.static   = torch.from_numpy(X[:, 66:])\n        self.target   = torch.from_numpy(y).view(-1, 1)\n    \n    def __len__(self): \n        return len(self.target)\n    \n    def __getitem__(self, idx):\n        return self.temporal[idx], self.static[idx], self.target[idx]\n\ndef create_dataloader(npz_path, batch_size=32, shuffle=True):\n    return DataLoader(RASSDataset(npz_path), batch_size=batch_size, shuffle=shuffle)\n\n# ---------- 2. 极简模型（LSTM + 静态特征拼接）----------\nclass SimpleRASSModel(nn.Module):\n    def __init__(self, hidden_dim=64):\n        super().__init__()\n        # 处理时序特征：11维 → hidden_dim\n        self.lstm = nn.LSTM(input_size=11, hidden_size=hidden_dim, batch_first=True)\n        # 处理静态特征：4维 → hidden_dim\n        self.static_fc = nn.Linear(4, hidden_dim)\n        # 拼接后输出1维\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n    \n    def forward(self, temporal, static):\n        # temporal: (batch, 6, 11)\n        # static:   (batch, 4)\n        lstm_out, (hn, cn) = self.lstm(temporal)\n        last_hidden = hn[-1]                # (batch, hidden_dim)\n        static_emb = self.static_fc(static) # (batch, hidden_dim)\n        combined = torch.cat([last_hidden, static_emb], dim=1)  # (batch, hidden_dim*2)\n        out = self.fc(combined)            # (batch, 1)\n        return out\n\n# ---------- 3. 训练（请修改路径）----------\nnpz_path = '/kaggle/input/nbp174/training_data_no_bp.npz'  # ⚠️ 修改成你的实际路径\n\n# 检查路径\nif not os.path.exists(npz_path):\n    print(\"❌ 文件不存在！当前目录下文件：\")\n    !ls /kaggle/input/nbp174/\n    raise FileNotFoundError(\"请将上面的 npz_path 改为正确的文件名\")\n\n# 加载数据\ntrain_loader = create_dataloader(npz_path, batch_size=32, shuffle=True)\n\n# 设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# 模型、优化器、损失\nmodel = SimpleRASSModel(hidden_dim=64).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# 训练循环\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for temporal, static, target in train_loader:\n        temporal = temporal.to(device)\n        static   = static.to(device)\n        target   = target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(temporal, static)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * len(target)\n    \n    avg_loss = total_loss / len(train_loader.dataset)\n    train_losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n# 画图\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\n# 保存模型\ntorch.save(model.state_dict(), 'simple_rass_model.pth')\nprint(\"✅ 模型已保存为 simple_rass_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T14:50:28.232435Z","iopub.execute_input":"2026-02-11T14:50:28.232832Z","iopub.status.idle":"2026-02-11T14:50:29.785370Z","shell.execute_reply.started":"2026-02-11T14:50:28.232799Z","shell.execute_reply":"2026-02-11T14:50:29.784538Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nEpoch 1/50, Loss: 3.7104\nEpoch 2/50, Loss: 3.5993\nEpoch 3/50, Loss: 3.9765\nEpoch 4/50, Loss: 3.4356\nEpoch 5/50, Loss: 3.4582\nEpoch 6/50, Loss: 3.2330\nEpoch 7/50, Loss: 3.2259\nEpoch 8/50, Loss: 3.2405\nEpoch 9/50, Loss: 3.2124\nEpoch 10/50, Loss: 3.1993\nEpoch 11/50, Loss: 3.1753\nEpoch 12/50, Loss: 3.0607\nEpoch 13/50, Loss: 3.0452\nEpoch 14/50, Loss: 3.2697\nEpoch 15/50, Loss: 3.1858\nEpoch 16/50, Loss: 3.1281\nEpoch 17/50, Loss: 3.0313\nEpoch 18/50, Loss: 2.9520\nEpoch 19/50, Loss: 2.9234\nEpoch 20/50, Loss: 2.9335\nEpoch 21/50, Loss: 3.1343\nEpoch 22/50, Loss: 2.8590\nEpoch 23/50, Loss: 2.7617\nEpoch 24/50, Loss: 3.2694\nEpoch 25/50, Loss: 2.8792\nEpoch 26/50, Loss: 2.7933\nEpoch 27/50, Loss: 2.9874\nEpoch 28/50, Loss: 2.6624\nEpoch 29/50, Loss: 2.6241\nEpoch 30/50, Loss: 2.5809\nEpoch 31/50, Loss: 2.6043\nEpoch 32/50, Loss: 2.7417\nEpoch 33/50, Loss: 2.7333\nEpoch 34/50, Loss: 2.5037\nEpoch 35/50, Loss: 2.6791\nEpoch 36/50, Loss: 2.4055\nEpoch 37/50, Loss: 2.3573\nEpoch 38/50, Loss: 2.4556\nEpoch 39/50, Loss: 2.3598\nEpoch 40/50, Loss: 2.3332\nEpoch 41/50, Loss: 2.3888\nEpoch 42/50, Loss: 2.4035\nEpoch 43/50, Loss: 2.6366\nEpoch 44/50, Loss: 2.3459\nEpoch 45/50, Loss: 2.2035\nEpoch 46/50, Loss: 2.3395\nEpoch 47/50, Loss: 2.2708\nEpoch 48/50, Loss: 2.2888\nEpoch 49/50, Loss: 2.2651\nEpoch 50/50, Loss: 2.1159\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAj8AAAHHCAYAAABQhTneAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAd/ZJREFUeJzt3Xd4lFXaBvD7nZJJ770SEkgIEEpACEiRXkRQ7LiIZVVAV9Z1XfETBcQFUVdlXUEFxYYorKCyUgISEAi9t9BCAqSH9DKZzLzfH5MZMmSSzCRTUu7fdeWSeducOcTk4ZznPEcQRVEEERERUQchsXcDiIiIiGyJwQ8RERF1KAx+iIiIqENh8ENEREQdCoMfIiIi6lAY/BAREVGHwuCHiIiIOhQGP0RERNShMPghIiKiDoXBDxHZ3YwZM9CpU6dm3Tt//nwIgmDZBhFRu8bgh4gaJAiCSV/Jycn2bqpdzJgxA66urvZuBhGZSeDeXkTUkG+//dbg9ddff42kpCR88803BsdHjx6NgICAZr+PSqWCRqOBQqEw+96amhrU1NTA0dGx2e/fXDNmzMD69etRVlZm8/cmouaT2bsBRNR6PfbYYwav9+/fj6SkpHrHb1dRUQFnZ2eT30culzerfQAgk8kgk/FHGRGZjtNeRNQiw4cPR48ePXDkyBEMHToUzs7OeO211wAAP//8MyZOnIjg4GAoFApERUXhrbfeglqtNnjG7Tk/V69ehSAIeO+99/DZZ58hKioKCoUC/fv3x6FDhwzuNZbzIwgCnn/+eWzcuBE9evSAQqFA9+7dsWXLlnrtT05ORr9+/eDo6IioqCh8+umnFs8jWrduHRISEuDk5ARfX1889thjuHHjhsE12dnZeOKJJxAaGgqFQoGgoCBMnjwZV69e1V9z+PBhjB07Fr6+vnByckJkZCSefPJJi7WTqKPgP5eIqMUKCgowfvx4PPzww3jsscf0U2CrV6+Gq6srXnrpJbi6uuL333/HG2+8gZKSErz77rtNPnfNmjUoLS3Fs88+C0EQsHTpUtx33324cuVKk6NFe/bswU8//YRZs2bBzc0Ny5Ytw9SpU5GRkQEfHx8AwLFjxzBu3DgEBQVhwYIFUKvVWLhwIfz8/FreKbVWr16NJ554Av3798fixYuRk5ODjz76CHv37sWxY8fg6ekJAJg6dSrOnDmDF154AZ06dUJubi6SkpKQkZGhfz1mzBj4+fnh1VdfhaenJ65evYqffvrJYm0l6jBEIiITzZ49W7z9x8awYcNEAOKKFSvqXV9RUVHv2LPPPis6OzuLVVVV+mOPP/64GBERoX+dlpYmAhB9fHzEmzdv6o///PPPIgDx119/1R97880367UJgOjg4CBeunRJf+zEiRMiAPHf//63/tikSZNEZ2dn8caNG/pjFy9eFGUyWb1nGvP444+LLi4uDZ6vrq4W/f39xR49eoiVlZX645s2bRIBiG+88YYoiqJYWFgoAhDffffdBp+1YcMGEYB46NChJttFRI3jtBcRtZhCocATTzxR77iTk5P+z6WlpcjPz8eQIUNQUVGB8+fPN/nchx56CF5eXvrXQ4YMAQBcuXKlyXtHjRqFqKgo/ev4+Hi4u7vr71Wr1di+fTumTJmC4OBg/XXR0dEYP358k883xeHDh5Gbm4tZs2YZJGRPnDgRsbGx+N///gdA208ODg5ITk5GYWGh0WfpRog2bdoElUplkfYRdVQMfoioxUJCQuDg4FDv+JkzZ3DvvffCw8MD7u7u8PPz0ydLFxcXN/nc8PBwg9e6QKihAKGxe3X36+7Nzc1FZWUloqOj611n7FhzpKenAwBiYmLqnYuNjdWfVygUeOedd7B582YEBARg6NChWLp0KbKzs/XXDxs2DFOnTsWCBQvg6+uLyZMn48svv4RSqbRIW4k6EgY/RNRidUd4dIqKijBs2DCcOHECCxcuxK+//oqkpCS88847AACNRtPkc6VSqdHjogkVOlpyrz3MmTMHFy5cwOLFi+Ho6Ih58+ahW7duOHbsGABtEvf69euRkpKC559/Hjdu3MCTTz6JhIQELrUnMhODHyKyiuTkZBQUFGD16tV48cUXcffdd2PUqFEG01j25O/vD0dHR1y6dKneOWPHmiMiIgIAkJqaWu9camqq/rxOVFQU/va3v2Hbtm04ffo0qqur8f777xtcM3DgQLz99ts4fPgwvvvuO5w5cwZr1661SHuJOgoGP0RkFbqRl7ojLdXV1fjkk0/s1SQDUqkUo0aNwsaNG5GZmak/funSJWzevNki79GvXz/4+/tjxYoVBtNTmzdvxrlz5zBx4kQA2rpIVVVVBvdGRUXBzc1Nf19hYWG9UavevXsDAKe+iMzEpe5EZBWDBg2Cl5cXHn/8cfzlL3+BIAj45ptvWtW00/z587Ft2zYMHjwYM2fOhFqtxscff4wePXrg+PHjJj1DpVJh0aJF9Y57e3tj1qxZeOedd/DEE09g2LBheOSRR/RL3Tt16oS//vWvAIALFy5g5MiRePDBBxEXFweZTIYNGzYgJycHDz/8MADgq6++wieffIJ7770XUVFRKC0txeeffw53d3dMmDDBYn1C1BEw+CEiq/Dx8cGmTZvwt7/9Da+//jq8vLzw2GOPYeTIkRg7dqy9mwcASEhIwObNm/Hyyy9j3rx5CAsLw8KFC3Hu3DmTVqMB2tGsefPm1TseFRWFWbNmYcaMGXB2dsaSJUvwj3/8Ay4uLrj33nvxzjvv6FdwhYWF4ZFHHsGOHTvwzTffQCaTITY2Fj/++COmTp0KQJvwfPDgQaxduxY5OTnw8PDAHXfcge+++w6RkZEW6xOijoB7exER3WbKlCk4c+YMLl68aO+mEJEVMOeHiDq0yspKg9cXL17Eb7/9huHDh9unQURkdRz5IaIOLSgoCDNmzEDnzp2Rnp6O5cuXQ6lU4tixY+jSpYu9m0dEVsCcHyLq0MaNG4fvv/8e2dnZUCgUSExMxD//+U8GPkTtGEd+iIiIqENhzg8RERF1KAx+iIiIqENhzo8RGo0GmZmZcHNzgyAI9m4OERERmUAURZSWliI4OBgSScPjOwx+jMjMzERYWJi9m0FERETNcO3aNYSGhjZ4nsGPEW5ubgC0nefu7m6x56pUKmzbtg1jxoyBXC632HPJOPa3bbG/bY99blvsb9tqTn+XlJQgLCxM/3u8IQx+jNBNdbm7u1s8+HF2doa7uzv/x7EB9rdtsb9tj31uW+xv22pJfzeVssKEZyIiIupQGPwQERFRh8Lgh4iIiDoUBj9ERETUoTD4ISIiog6l1QQ/S5YsgSAImDNnTqPXrVu3DrGxsXB0dETPnj3x22+/GZwXRRFvvPEGgoKC4OTkhFGjRuHixYtWbDkRERG1Ja0i+Dl06BA+/fRTxMfHN3rdvn378Mgjj+Cpp57CsWPHMGXKFEyZMgWnT5/WX7N06VIsW7YMK1aswIEDB+Di4oKxY8eiqqrK2h+DiIiI2gC7Bz9lZWWYNm0aPv/8c3h5eTV67UcffYRx48bh73//O7p164a33noLffv2xccffwxAO+rz4Ycf4vXXX8fkyZMRHx+Pr7/+GpmZmdi4caMNPg0RERG1dnYPfmbPno2JEydi1KhRTV6bkpJS77qxY8ciJSUFAJCWlobs7GyDazw8PDBgwAD9NURERNSx2bXC89q1a3H06FEcOnTIpOuzs7MREBBgcCwgIADZ2dn687pjDV1jjFKphFKp1L8uKSkBoK0uqVKpTGqbKXTPsuQzqWHsb9tif9se+9y22N+21Zz+NvVauwU/165dw4svvoikpCQ4OjraqxkAgMWLF2PBggX1jm/btg3Ozs4Wf7+kpCSLP5Maxv62Lfa37bHPbYv9bVvm9HdFRYVJ19kt+Dly5Ahyc3PRt29f/TG1Wo3du3fj448/hlKphFQqNbgnMDAQOTk5BsdycnIQGBioP687FhQUZHBN7969G2zL3Llz8dJLL+lf6zZGGzNmjMX39kpKSsLo0aO5L4wNsL9ti/1te+xz22J/21Zz+ls3c9MUuwU/I0eOxKlTpwyOPfHEE4iNjcU//vGPeoEPACQmJmLHjh0Gy+GTkpKQmJgIAIiMjERgYCB27NihD3ZKSkpw4MABzJw5s8G2KBQKKBSKesflcrlVvsEt8dwqlRoOUgkkksY3byPr/T2Scexv22Of2xb727bM6W9Tr7Nb8OPm5oYePXoYHHNxcYGPj4/++PTp0xESEoLFixcDAF588UUMGzYM77//PiZOnIi1a9fi8OHD+OyzzwBAXydo0aJF6NKlCyIjIzFv3jwEBwdjypQpNv181lRYXo273k9GQrgXVs3ob+/mEBERtSl2TXhuSkZGBiSSWwvSBg0ahDVr1uD111/Ha6+9hi5dumDjxo0GQdQrr7yC8vJyPPPMMygqKsKdd96JLVu22D2vyJJOXC9CUYUKKVcK7N0UIiKiNqdVBT/JycmNvgaABx54AA888ECDzxAEAQsXLsTChQst3LrW49pNbUJXRbUa5coauCha1V8jERFRq2b3Oj9kvvSCW9nseaXKRq4kIiKi2zH4aYMybtYJfsoY/BAREZmDwU8bZBD8cOSHiIjILAx+2hhRFBn8EBERtQCDnzYmv6waFdVq/evcUu5WT0REZA4GP21Mxs1yg9cc+SEiIjIPg582pu6UF8Dgh4iIyFwMftoY3TJ3Pzftdhxc7UVERGQeBj9tjG7kp1+EFwCO/BAREZmLwU8bk1E78pNQG/zkl1VDoxHt2SQiIqI2hcFPG5NeO/LTJ1wb/Kg1Igorqu3ZJCIiojaFwU8bUlmt1k9zRfm5wNvFAQDzfoiIiMzB4KcN0eX7uDvK4OnsAD/X2qRn5v0QERGZjMFPG6ILfiJ8XAAA/u7a4Ce3hMEPERGRqRj8tCHpBdoCh+HezgBwa+SH015EREQmY/DThuhGfsJ9aoMfN057ERERmYvBTxuiD368GfwQERE1F4OfNkRX4yeCwQ8REVGzMfhpI9QaEdcLKwEAYcz5ISIiajYGP21EdkkVqtUayKUCgj2dAHDkh4iIqDkY/LQRuimvUC9nSCUCgFvBT3GlCsoatd3aRkRE1JYw+GkjMm5ql7nrprwAwMNJDgep9q+Qoz9ERESmYfDTRqTfluwMAIIgcOqLiIjITAx+2ojbl7nr+DL4ISIiMguDnzbi9gKHOlzxRUREZB4GP21EQyM/nPYiIiIyD4OfNqC4UoWiChUABj9EREQtxeCnDdAtc/d1VcBFITM4x+CHiIjIPAx+2oBbU15O9c4x54eIiMg8DH7agPTaGj8RPi71znHkh4iIyDwMftqAa7UjP2G35fsAgH9t8JNbqoQoijZtFxERUVvE4KcNMFbgUEc38lNdo0FJVY1N20VERNQWMfhpAxqq8QMAjnIp3By1SdCc+iIiImoag59WrrpGg8yiSgDGR34A5v0QERGZg8FPK3ejqBIaEXCUS/RBzu244ouIiMh0dg1+li9fjvj4eLi7u8Pd3R2JiYnYvHlzg9cPHz4cgiDU+5o4caL+mhkzZtQ7P27cOFt8HKuoW9lZEASj13Dkh4iIyHSypi+xntDQUCxZsgRdunSBKIr46quvMHnyZBw7dgzdu3evd/1PP/2E6upq/euCggL06tULDzzwgMF148aNw5dffql/rVAYHzFpCzIKtMvcw73rL3PXYfBDRERkOrsGP5MmTTJ4/fbbb2P58uXYv3+/0eDH29vb4PXatWvh7OxcL/hRKBQIDAy0fIPtoKE9vepi8ENERGQ6uwY/danVaqxbtw7l5eVITEw06Z5Vq1bh4YcfhouL4ahIcnIy/P394eXlhREjRmDRokXw8fFp8DlKpRJK5a3AoaSkBACgUqmgUqma8WmM0z3LnGdezdeO/IR6Khq8z8dZ+9eYW1Jp0fa2dc3pb2o+9rftsc9ti/1tW83pb1OvFUQ7V8Y7deoUEhMTUVVVBVdXV6xZswYTJkxo8r6DBw9iwIABOHDgAO644w79cd1oUGRkJC5fvozXXnsNrq6uSElJgVQqNfqs+fPnY8GCBfWOr1mzBs7ODY+42MKSE1JkVQh4JlaN7l7G/6rOFQlYcU6KYGcR/+iltnELiYiIWoeKigo8+uijKC4uhru7e4PX2T34qa6uRkZGBoqLi7F+/XqsXLkSu3btQlxcXKP3Pfvss0hJScHJkycbve7KlSuIiorC9u3bMXLkSKPXGBv5CQsLQ35+fqOdZy6VSoWkpCSMHj0acrm8yetFUUTvRb+jolqNrX8ZjM5+xvN+zmWV4p5PUuDj4oD9rw63WHvbOnP7m1qG/W177HPbYn/bVnP6u6SkBL6+vk0GP3af9nJwcEB0dDQAICEhAYcOHcJHH32ETz/9tMF7ysvLsXbtWixcuLDJ53fu3Bm+vr64dOlSg8GPQqEwmhQtl8ut8g1u6nPzSpWoqFZDEIBO/m6Qy4yPXAV5aYOimxXVECRSyKSsYFCXtf4eyTj2t+2xz22L/W1b5vS3qde1ut+SGo3GYBTGmHXr1kGpVOKxxx5r8nnXr19HQUEBgoKCLNVEm9ElOwe5O0LRQOADAN4uDpAIgCgCN8urG7yOiIiI7Bz8zJ07F7t378bVq1dx6tQpzJ07F8nJyZg2bRoAYPr06Zg7d269+1atWoUpU6bUS2IuKyvD3//+d+zfvx9Xr17Fjh07MHnyZERHR2Ps2LE2+UyWlFG7m7uxbS3qkkoE+Lje2uCUiIiIGmbXaa/c3FxMnz4dWVlZ8PDwQHx8PLZu3YrRo0cDADIyMiCRGMZnqamp2LNnD7Zt21bveVKpFCdPnsRXX32FoqIiBAcHY8yYMXjrrbdaTa2fG+XaLStMGZnLKNBua9HYMncdP1cF8kqVrPJMRETUBLsGP6tWrWr0fHJycr1jMTExaChH28nJCVu3brVE06zilf+ewoaTMvhGZ+PBOyKavD69duQnwqfhAoc6fm4KIIu1foiIiJrS6nJ+2rMoP1cAwMo9VxsM4OrKKNDm/ISZMPLjz0KHREREJmHwY0OP9A+FQiLiQm4Zki/kNXm9LuG5od3c62KVZyIiItMw+LEhdyc5EgO0Iz6f7rrc6LWV1Wp98nJEEwnPAIMfIiIiUzH4sbHhQRrIJAL2X7mJk9eLGrzuWqF21MfNUQYPp6azoxn8EBERmYbBj415KYC7e2o3Xf1095UGr0uvzfeJ8HGGIAhNPtevdqk7V3sRERE1jsGPHTx9ZycAwOZTWfqk5tulF9TW+DEh3wfgyA8REZGpGPzYQUygG4Z19YNGBFbuMT76c6022Tncu+ll7sCt4KdMWYOK6hrLNJSIiKgdYvBjJ88O7QwA+PHwNaNbUqTrgx/TRn5cFTI4yrV/nfml3OKCiIioIQx+7CQxygc9QzxQpdLg65Sr9c7rl7mbsNILAARBuDX1VVZlsXYSERG1Nwx+7EQQBDxTO/rzdUo6KqvV+nNqjYjrN03f2kLH380RAPN+iIiIGsPgx47G9whEmLcTbpZXY/2Ra/rjOSVVqFZrl8QHeTia/Dw/bm5KRETUJAY/diSTSvD0ndrRn8//SINaoy2AqFvmHurlBJnU9L8irvgiIiJqGoMfO3ugXyi8nOXIuFmBLaezAQAZtRuamrKnV10MfoiIiJrG4MfOnB1k+FNiJwDAZ7svQxRFs5OddRj8EBERNY3BTyvweGIEFDIJTlwvxoG0m/ppL3OSnQFWeSYiIjIFg59WwMdVgQf6hQLQbnhqboFDHY78EBERNY3BTyvx9J2dIQjAztQ8nMsuBdD8aa/8MiU0tcnTREREZIjBTyvRydcF43toNzytrtEAMD/h2bd22kulFlFcqbJsA4mIiNoJBj+tyDNDo/R/9nV1gKtCZtb9DjIJvJzlAJj3Q0RE1BAGP61I7zBPDIj0BmD+qI+Obuort4TBDxERkTEMflqZv42JgZNcitFxAc26n/t7ERERNc68eRWyujsivXF6wVhIJUKz7tcvd+eKLyIiIqM48tMKNTfwAbjcnYiIqCkMftoZBj9ERESNY/DTztzK+WHwQ0REZAyDn3bG380RAEd+iIiIGsLgp53htBcREVHjGPy0M7rVXoUVKn2laCIiIrqFwU874+Ekh1yqXS2Wz7wfIiKiehj8tDMSiaDf44tTX0RERPUx+GmHmPdDRETUMAY/7ZC+yjOnvYiIiOph8NMOceSHiIioYQx+2iF/Bj9EREQNsmvws3z5csTHx8Pd3R3u7u5ITEzE5s2bG7x+9erVEATB4MvR0dHgGlEU8cYbbyAoKAhOTk4YNWoULl68aO2P0qpw5IeIiKhhdg1+QkNDsWTJEhw5cgSHDx/GiBEjMHnyZJw5c6bBe9zd3ZGVlaX/Sk9PNzi/dOlSLFu2DCtWrMCBAwfg4uKCsWPHoqqqytofp9XgFhdEREQNk9nzzSdNmmTw+u2338by5cuxf/9+dO/e3eg9giAgMDDQ6DlRFPHhhx/i9ddfx+TJkwEAX3/9NQICArBx40Y8/PDDlv0ArZQu+Mkt7TgBHxERkansGvzUpVarsW7dOpSXlyMxMbHB68rKyhAREQGNRoO+ffvin//8pz5QSktLQ3Z2NkaNGqW/3sPDAwMGDEBKSkqDwY9SqYRSeWuUpKSkBACgUqmgUqks8fH0z6v7X2vxdJQC0E57VVdXQxAEq75fa2Wr/iYt9rftsc9ti/1tW83pb1OvtXvwc+rUKSQmJqKqqgqurq7YsGED4uLijF4bExODL774AvHx8SguLsZ7772HQYMG4cyZMwgNDUV2djYAICAgwOC+gIAA/TljFi9ejAULFtQ7vm3bNjg7O7fg0xmXlJRk8WfWpVQDgAxVKg02/LoZjnb/W7Yva/c3GWJ/2x773LbY37ZlTn9XVFSYdJ0giqLY3AZZQnV1NTIyMlBcXIz169dj5cqV2LVrV4MBUF0qlQrdunXDI488grfeegv79u3D4MGDkZmZiaCgIP11Dz74IARBwA8//GD0OcZGfsLCwpCfnw93d/eWf8g67U1KSsLo0aMhl8st9lxjei/agXKlGtteHIxIXxervldrZcv+Jva3PbDPbYv9bVvN6e+SkhL4+vqiuLi40d/fdh8TcHBwQHR0NAAgISEBhw4dwkcffYRPP/20yXvlcjn69OmDS5cuAYA+FygnJ8cg+MnJyUHv3r0bfI5CoYBCoTD6fGt8g1vruXX5uzkiTVmOwko1unbw/0lt0d90C/vb9tjntsX+ti1z+tvU61pdnR+NRmMwCtMYtVqNU6dO6QOdyMhIBAYGYseOHfprSkpKcODAgUbziNojVnkmIiIyzq4jP3PnzsX48eMRHh6O0tJSrFmzBsnJydi6dSsAYPr06QgJCcHixYsBAAsXLsTAgQMRHR2NoqIivPvuu0hPT8fTTz8NQLsSbM6cOVi0aBG6dOmCyMhIzJs3D8HBwZgyZYq9PqZd+Lmz1g8REZExdg1+cnNzMX36dGRlZcHDwwPx8fHYunUrRo8eDQDIyMiARHJrcKqwsBB//vOfkZ2dDS8vLyQkJGDfvn0G+UGvvPIKysvL8cwzz6CoqAh33nkntmzZUq8YYnvnx53diYiIjLJr8LNq1apGzycnJxu8/uCDD/DBBx80eo8gCFi4cCEWLlzY0ua1aaZUeRZFEXsu5SO/TIkpvUM67JJ4IiLqWOye8EzWcavQYf3gp0qlxsZjN/DF3jRcyCkDAHg4yTEiNqDetURERO0Ng592ytjIT16pEt/sT8e3+9Nxs7za4Pr9V24y+CEiog6BwU87VXe11/nsEqz6Iw0/H89EtVoDAAjxdMITgztBJhEw/9ezOJJeaM/mEhER2QyDn3bKv87Iz7gP/9Af7xPuiafv7Iyx3QMgk0pwNb8c+PUsTl0vhrJGDYVMaq8mExER2QSDn3bK28UBcqkAlVqERADG9wjCk3dGIiHCy+C6CB9n+Lg4oKC8GqdvlNQ7T0RE1N4w+GmnZFIJ3p7SExk3K/BQ/zCEeRvfo0wQBPSN8ELS2RwcTS9k8ENERO0eg5927MH+YSZdl1Ab/BxJL8SfrdwmIiIie2t121uQ7elGe45kFMLO+9wSERFZHYMfQs8QD8ilAvJKlbheWGnv5hAREVkVgx+Co1yKuGAPAOCSdyIiavcY/BAAICG8duqLwQ8REbVzDH4IwK28n6MZDH6IiKh9Y/BDAIC+EZ4AgHNZJShX1ti3MURERFbE4IcAAEEeTgjxdIJGBE5cK7J3c4iIiKyGwQ/p9Y1g3g8REbV/DH5ILyHcE4C23g8REVF7xeCH9BIivAEAR9MLodGw2CEREbVPDH5ILzbIDU5yKUqqanA5r8zezSEiIrIKBj+kJ5dK0CuMxQ6JiKh9Y/BDBhKY9ExERO0cgx8yUHeTUyIiovaIwQ8Z6BOmDX6u5JXjZnm1nVtDRERkeQx+yICXiwOi/FwAAMc4+kNERO0Qgx+qh3k/RETUnjH4oXoY/BARUXvG4Ifq0QU/J64XQaXW2Lk1RERElsXgh+rp7OsKDyc5qlQanMsqsXdziIiILIrBD9UjkQjoq9vni1NfRETUzjD4IaN0U19HM4rs2xAiIiILY/BDRvXVBT8c+SEionaGwQ8Z1SvUE1KJgBtFlcgqrrR3c4iIiCyGwQ8Z5aKQoVuQGwDgaHqRfRtDRERkQQx+qEEJ4az3Q0RE7Q+DH2pQX25yapaT14vwx8U8ezejw0jNLsW+y/n2bgYRtUF2DX6WL1+O+Ph4uLu7w93dHYmJidi8eXOD13/++ecYMmQIvLy84OXlhVGjRuHgwYMG18yYMQOCIBh8jRs3ztofpV3Srfg6c6MYVSq1nVvTuqnUGjy28gD+tOog9l3iL2RbmPHlQTy28gCyi6vs3RQiamPsGvyEhoZiyZIlOHLkCA4fPowRI0Zg8uTJOHPmjNHrk5OT8cgjj2Dnzp1ISUlBWFgYxowZgxs3bhhcN27cOGRlZem/vv/+e1t8nHYnxNMJAe4K1GhEnLxebO/mtGrnskpQUlUDAJj382lU17AytjVVVNcgq7gKGhG4lFtm7+YQURtj1+Bn0qRJmDBhArp06YKuXbvi7bffhqurK/bv32/0+u+++w6zZs1C7969ERsbi5UrV0Kj0WDHjh0G1ykUCgQGBuq/vLy8bPFx2h1BELjPl4nq9s/lvHKs3HPFjq1p/3JLlPo/XyussGNLiKgtktm7ATpqtRrr1q1DeXk5EhMTTbqnoqICKpUK3t7eBseTk5Ph7+8PLy8vjBgxAosWLYKPj0+Dz1EqlVAqb/0wLSnRbumgUqmgUqma8WmM0z3Lks+0tt6hHvjtVDYOXy2AShVu7+aYxZb9fTjtJgAgLsgNZ7NK8e8dFzGhuz9CPJ2s/t6thS37O7OwXP/nq3llber/KUtqiz9T2jL2t201p79NvVYQRVFsVqss5NSpU0hMTERVVRVcXV2xZs0aTJgwwaR7Z82aha1bt+LMmTNwdHQEAKxduxbOzs6IjIzE5cuX8dprr8HV1RUpKSmQSqVGnzN//nwsWLCg3vE1a9bA2dm5+R+uHbhaCnxwWgYXmYi3+6khCM1/ligCRdWAswxQGP+raLPePCJFUbWA2XFqbLkmweVSAT29NHg6ltNf1nA0X8BXF7XfRH18NJjRlf1MRNpBkUcffRTFxcVwd3dv8Dq7Bz/V1dXIyMhAcXEx1q9fj5UrV2LXrl2Ii4tr9L4lS5Zg6dKlSE5ORnx8fIPXXblyBVFRUdi+fTtGjhxp9BpjIz9hYWHIz89vtPPMpVKpkJSUhNGjR0Mul1vsudZUXaNBn7d/R3WNBklzBqOTj4tZ91ep1DiQdhO7LuQj+UI+rhVWws1RhlnDOuNPA8KgkFsvCrJVf2cVV2Hoe7shlQg4+n934XphJSZ/sh81GhGfPdYHd8X4We29WxNbfn9/uS8d/9ycCgCID3HHf58baNX3a63a4s+Utoz9bVvN6e+SkhL4+vo2GfzYfdrLwcEB0dHRAICEhAQcOnQIH330ET799NMG73nvvfewZMkSbN++vdHABwA6d+4MX19fXLp0qcHgR6FQQKFQ1Dsul8ut8g1uredag1wOxId44HB6IV5efxrxoZ6I8HFGJx8XdPJ1RqiXMxxvC2DSC8qx83wuki/kIeVyAZS3Jf+WVtXgna0X8O2Ba3hlXAwmxQdDImnBkFKTn8G6/X3ihnZ5e1yQOzxcnODh4oQn74zEZ7uv4K3fzmNoTEC9PmrPbPH9nV9+a2j7elFVm/n/yVra0s+U9oD9bVvm9Lep19k9+LmdRqMxGIW53dKlS/H2229j69at6NevX5PPu379OgoKChAUFGTJZnYod8X643B6IU5cL8aJ21Z9CQIQ7OGECB9nBLo74vi1IlzJLze4JtjDEcNj/TG8qx8GRvlg6+lsvL/tAm4UVeLFtcexak8aXpvQDQM7N5yX1Zrpkp11yeEA8OLILvjleCau3azEJ8mX8dLorvZqXruUW3JrefvN8mqUKWvgqmh1P86IqJWy60+LuXPnYvz48QgPD0dpaSnWrFmD5ORkbN26FQAwffp0hISEYPHixQCAd955B2+88QbWrFmDTp06ITs7GwDg6uoKV1dXlJWVYcGCBZg6dSoCAwNx+fJlvPLKK4iOjsbYsWPt9jnbupnDojCwszcu55UjvaAcV/MrcLWgHOkFFShT1uBGUSVuFN3a/0smEdCvkxfuivHH8Bh/dA1whVAnWeiBfmG4Oz4YX+xNw/Lkyzh5vRgPf7Yfo7r549XxsYj2d7PHx2y2o7VFIPvWCX5cFDK8MSkOs747ihXJl3FvnxBE+po3ZUgNyykx/AfStZsV6BZkuSlqImrf7Br85ObmYvr06cjKyoKHhwfi4+OxdetWjB49GgCQkZEBieTWavzly5ejuroa999/v8Fz3nzzTcyfPx9SqRQnT57EV199haKiIgQHB2PMmDF46623jE5rkWkkEgEJEd5IiDBcVSeKIgrKq/UBUWZRJboEuGJwtC/cHBsfenRykGL2XdF4qH8YPtp+EWsOZmD7uVzsTM3Dw/3D8OKoLvB3c7Tmx7KIiuoanMnUrg6sO/IDAON7BGJoVz/svpCHN385g6+e6G8QBFLz5ZRqR34EQZtIz+CHiMxhdvBTWVkJURT1q6DS09OxYcMGxMXFYcyYMWY9a9WqVY2eT05ONnh99erVRq93cnLSjxqR9QmCAF9XBXxdFfUCI1P5uirw1pQemDG4E5ZsPo+kszn47kAGvj+YgR4hHkiM8sGgKF/07+QFZ4fWN61x8nox1BoRge6OCPYwDNYEQcDCe7pjzIe7sftCHjafzsaEnpx+tYS82pGfmAA3nM8uRcZN1vohItOZXeRw8uTJ+PrrrwEARUVFGDBgAN5//31MnjwZy5cvt3gDqWOI8nPF59P74YdnBqJvuCc0ojaw+HTXFTz+xUH0WrAND6zYh38lXcD+KwVQ1rSO7Tbq5vsYG9Xp5OuC54ZFAQAW/noWZcoam7avPSpX1qC0th/7d9IG3dcLKxu7hYjIgNnBz9GjRzFkyBAAwPr16xEQEID09HR8/fXXWLZsmcUbSB3LgM4++GnWYOyfOxIfPNQLDySEIsTTCSq1iENXC7Fsx0U8/Nl+xM/fhse/OIid53Nhz2oNR9Pr5/vcbtbwKIR7OyO7pArLdly0VdPardxS7aiPs4MUsUHa/DCO/BCROcyeR6ioqICbm/YHzrZt23DfffdBIpFg4MCBSE9Pt3gDqWMK9HDEvX1CcW+fUIiiiGs3K5FyJR/7Lhdg3+UC5JUqsetCHnZdyEO3IHfMHB6FCT0CIZPabscWjUbU73h/e75PXY5yKRbc0x1PrD6EL/akYWrfUMQEtq2k7tZEt9IrwN0RYV7a6fdrDH6IyAxm/6aIjo7Gxo0bce3aNWzdulWf55Obm2vRgoBEOoIgINzHGQ/1D8dHD/fBwddGIumvQ/HM0M5wcZDiXFYJ/vL9MYx4fxe+O5Busx3or+SXo6hCBUe5BN2DG//evyvWH2O7B6BGI2Lez6ftOlrV1uXUjvz4uykQ7l0b/BRWsE+JyGRmBz9vvPEGXn75ZXTq1AkDBgzQ78O1bds29OnTx+INJLqdIAjoEuCG1yZ0w95XR+Cl0V3h5SxHxs0K/N+G0xiydCdW7LqM0irr5tfoprziQz0hN2HE6Y1J3eEkl+Jg2k18knzZqm1rz3QjP/7ujgj2dIIgAFUqDfLKGq4PRkRUl9nBz/3334+MjAwcPnwYW7Zs0R8fOXIkPvjgA4s2jqgpns4O+MvILtj76gi8OSkOwR6OyCtVYsnm8xj2/m5sypBAaaWRIGPFDRsT4umE1+/uBgB4d2sqfjp63Srtau90OT8Bbgo4yCQI9tBuHnvtJpOeicg0zUqQCAwMRJ8+fSCRSFBSUoKNGzfCzc0NsbGxlm4fkUmcHWR4YnAkdr1yF957oBei/V1RWlWDpBsS/Gv7Jau8pz7fJ9y04AcApg2IwLPDOgMAXll/En9czLNK29qznDo5PwAQ6qULfpj3Q0SmMTv4efDBB/Hxxx8D0Nb86devHx588EHEx8fjv//9r8UbSGQOuVSC+xNCsW3OUCy8RzvK8sPh6yipUjVxp3mKKqpxKbcMQOMrvYz5x9hY3NMrGDUaETO/PYqztUUSyTQ5+mkvbeFSfd4Pgx8iMpHZwc/u3bv1S903bNgAURRRVFSEZcuWYdGiRRZvIFFzSCQCHu4XikAnEeXVavx46JpFn38sowgA0NnXBd4uDma37d0H4jGwszfKlDV4YvVBg+1BqHG5JbqEZ+3IT1ht8MPl7kRkKrODn+LiYnh7awuLbdmyBVOnToWzszMmTpyIixdZw4RaD0EQMDxIu6P8l3uvokataeIO0x0xob5PYxQyKT79Uz90DXBFTokSM744iOIKy45OtVf6nJ/akZ8w79ppr0IGP0RkGrODn7CwMKSkpKC8vBxbtmzRL3UvLCyEo2Pr34uJOpYEXxHeLnLcKKrEljPZFnvu4fSb2uc3M/gBAA8nOVY/cQcC3BW4mFuGZ7453GoqV7dWZcoafZVs/9qcn1vTXhw9IyLTmB38zJkzB9OmTUNoaCiCg4MxfPhwANrpsJ49e1q6fUQt4iAFpt0RBgD4/I80i9SCUak1OHGtGADQrwXBDwAEezph9RN3wFUhw4G0m3h53UloNKxX0xDdMncXBylcFdoarbpCh1nFlVBZcHSPiNovs4OfWbNmISUlBV988QX27Nmj33W9c+fOzPmhVunRO8LgIJPgxLUiHK1dodUS57NKUalSw91Rhig/1xY/r1uQOz79UwJkEgG/nsjEO1vOt/iZ7dWtKa9bo8x+bgooZBJoRCCTuVNEZIJmLXXv168f7r33Xri4uOj/JT1x4kQMHjzYoo0jsgRfVwXu7R0CAFj5R1qLn3ekdsqrb4QXJJL6m5k2x+BoXyy9Px4A8OnuK1i9t+XttLXkC3n4v0NS7LlUYLX3uH2lF6DN7WLSMxGZo1nBz9dff42ePXvCyckJTk5OiI+PxzfffGPpthFZzFNDIgEAW89kt3hJ9JHalV7m1PcxxX19Q/H3sTEAgAWbziI5Ndeiz7e2bw9cQ1mNgI3HM632Hrev9NJh3g8RmcPs4Odf//oXZs6ciQkTJuDHH3/Ejz/+iHHjxuG5555jhWdqtboGuGFoVz9oRO3Kr5Y4amZlZ3PMGh6Fh/uHQRSBRf87B3Ubyf+pUWtwuLZfzmWVWu19ckt1BQ4VBsfDagsdcuSHiExhdvDz73//G8uXL8c777yDe+65B/fccw+WLl2KTz75BMuWLbNGG4ks4uk7taM/PxzKaHbRw6ziStwoqoREAHqFeVqwdVqCIOC1id3g6SzHpdyyNrMFxqkbxShXaleqXckvt9rmsjkl9XN+gFu1frjcnYhMYXbwk5WVhUGDBtU7PmjQIGRlZVmkUUTWMKSLL7oGuKK8Wo0fDjav6KGuvk+3IHe41K42sjR3RzlmDosCAHy4/WKbWP6ecuVWnk+NRtRXv7Y0Xc6Pn9ttIz+s8kxEZjA7+ImOjsaPP/5Y7/gPP/yALl26WKRRRNYgCAKeqh39+XJvWrOKHpq7mWlzPT6oEwLcFbhRVInvD2RY9b0sIeWyYZLz2SzrbNmRZ2S1F3BruTuDHyIyhdn/dF2wYAEeeugh7N69W7+6a+/evdixY4fRoIioNZncOwRLt6Qis7gKm09nY1KvYLPut2a+T12OcileGNEFr288jY93XsID/cKsNtLUUtU1Ghy+qu2Xrh4aXCiWWG2/sts3NdXRVXkurFChtEoFN0e5Vd6fiNoHs0d+pk6digMHDsDX1xcbN27Exo0b4evri4MHD+Lee++1RhuJLMZRLsWfEiMAACv/uGJW0cPKajXO1P5St3bwAwAP9Q9DhI8z8suq8WUrXvp+8noRKlVqeDnLkeCr7c9zVhj5KVPWoLxaOwXof9u0l5ujHF7O2oCHK76IqCnNWuqekJCAb7/9FkeOHMGRI0fw7bffIiQkBP/85z8t3T4ii3tsYIS26OH1Yv00lilOXi9CjUZEgLsCIZ5OVmyhllwqwUujuwLQ1v4pqqi2+ns2h27Ka0CkN0JdbgU/lqimXZdu1MdVITM6ChbOpGciMlGzgh9jsrKyMG/ePEs9jshqfF0VuK+Ptujhqj2mj6gcybg15SUIlilu2JRJ8cGIDXRDaVUNVuy6YpP3NJcu2XlgpBcCnQC5VEBJVY3Fd6rX1/i5bZm7TiiTnonIRBYLfojakifvvFX0MKPAtF+WunyfvhYubtgYiUTAy2O0hQ9X70vT723VWihr1PrRswGR3pBJgChfFwCWr/ejr/HjZnwD5XAGP0RkIgY/1CF1DXDDMF3Rw31Nj/6IomizlV63G9nNH33DPVGl0mDZ7xdt+t5NOZZRBGWNBn5uCkT5aYOebkFuAGDxpGdjW1vUpVvxxUKHRNQUBj/UYT1du+XFj4euobiy8aKHV/LLUVihgoNMgu7BHrZonp4gCHhlXCwAYO3Ba0gvKLfp+zdGl+8zsLOPfiowNlAb/Fg66Tm3gQKHOroVX9cKmfBMRI0zee3sSy+91Oj5vLy8FjeGyJbujPZFTIAbUnNK8X8bTmHGoE7oE+4FqZHNSnWjPr1CPeAgs/2/GQZ29sGQLr7442I+Pki6gA8f7mPzNhijy/dJ7OyjP6YPfrItPPJTqtvXy/jIT91pL1EUbZaXRURtj8nBz7Fjx5q8ZujQoS1qDJEtCYKAmcOjMOeH49h0MgubTmbB28UBI2L9MapbAIZ08dWvKtLn+9h4yquuV8bG4o+Le/DziUw8NzwKsYHudmsLAFSp1Dheu8lrYlT94Ce9oMKiNXduTXsZH/kJ9nSCRACUNRrklSobvI6IyOTgZ+fOndZsB5FdTOkTAheFDJtOZmLn+VzcLK/G+iPXsf7IdTjIJBgc5YNRcQE4kHYTANAvwttube0Z6oEJPQPx26lsvLf1AlY+3s9ubQG0o2HVag0C3R3RyccZNTU1AABvFwcEujsiu6QKqdml6NfJMn2mr+7cwMiPXCpBkIcTbhRV4lphBYMfImpQ6ywZS2RDo+MCMDouACq1Boeu3sT2s7lIOpeNazcrsTM1DztTb03p9g33tF9DAbw0OgZbTmdj+7kcHEkvtHnydV26fJ/EKJ96U0xxwe7ILqnC2awSiwQ/oig2WN25rjBvbfCTcbMCCXYMVImodWPCM1EtuVSCQVG+eGNSHHb//S5s++tQ/H1sDPqEe0IQtHktPq7GRx1sJdrfFVP7hgIA3t163uKFBM1hLN9HR7fiy1JJz2XKGlToqjs3sNoLqJv3w6RnImoYR36IjBAEAV0D3NA1wA2z74pGaZUKTnKpvZsFAJgzuit+Pp6J/VduYs+lfAzp4mfzNpQra3DiWhEAw3wfnW5B2nyksxaq9ZNTu9LLTSGDs0PDP7a43J2ITMGRHyITuDnKIZO2jv9dQjydMG1gOABg0aZzqK4xf3f6ljqcXogajYgQTyeE1Y621BVXG/ykZpdArWn56JSuwGFjoz4A9G1hoUMiakzr+GlORGZ5YUQXeLs4IDWnFCt2Xbb5+9fN9zEmwscFTnIpqlQapOW3vC5RUzV+dBj8EJEpTA5+li5disrKW/Poe/fuhVKp1L8uLS3FrFmzzHrz5cuXIz4+Hu7u7nB3d0diYiI2b97c6D3r1q1DbGwsHB0d0bNnT/z2228G50VRxBtvvIGgoCA4OTlh1KhRuHixdVXFJWopbxcHvDkpDgDw8e+XcCnXsltJNKWxfB8AkEoExOoqPVsg70e/zL2BlV46ukKHWSVVdhkRI6K2weTgZ+7cuSgtvfUDdvz48bhx44b+dUVFBT799FOz3jw0NBRLlizBkSNHcPjwYYwYMQKTJ0/GmTNnjF6/b98+PPLII3jqqadw7NgxTJkyBVOmTMHp06f11yxduhTLli3DihUrcODAAbi4uGDs2LGoqmpdeyIRtdQ9vYIxPMYP1WoNXv3vKWgsML1kitIqFU7fKAYADGxg5Ae4lfdjiaTn3FLTRn78XBVwlEsgikCmhTdWJaL2w+Tg5/ZVJZZYZTJp0iRMmDABXbp0QdeuXfH222/D1dUV+/fvN3r9Rx99hHHjxuHvf/87unXrhrfeegt9+/bFxx9/rG/Thx9+iNdffx2TJ09GfHw8vv76a2RmZmLjxo0tbi9RayIIAt6+tydcHKQ4nF6Ibw+k2+R9D129CbVGRLi3M0I8nRq8zpLBT1MFDnUEQWDSMxE1qdWs9lKr1Vi3bh3Ky8uRmJho9JqUlJR622yMHTtWH9ikpaUhOzsbo0aN0p/38PDAgAEDkJKSgocfftjoc5VKpcEUXkmJ9oe1SqWCStX4nk/m0D3Lks+khnWE/vZ3keFvo7tg4f/O453N5zG8iw+CPKxb3G/vRW3dowGRXgZ9e3t/x/hpg5CzmSUt/jvILtaO4vg4y5p8VqiXIy7mluFqfikSIz1b9L6tXUf4Hm9N2N+21Zz+NvVauwc/p06dQmJiIqqqquDq6ooNGzYgLi7O6LXZ2dkICAgwOBYQEIDs7Gz9ed2xhq4xZvHixViwYEG949u2bYOzc/2VLC2VlJRk8WdSw9p7f3uJQCdXKa6WqfHcymQ8E6uBNbe12npSCkCAY3EGfvut/miTrr+VakCAFLmlSvzw829wa8EuF1dztO955cxR/Hat8WvVxRIAEuw6fAYeeaea/6ZtSHv/Hm9t2N+2ZU5/V1SYNuJrVvCzcuVKuLq6AgBqamqwevVq+Pr6AoBBPpA5YmJicPz4cRQXF2P9+vV4/PHHsWvXrgYDIGuYO3euwYhSSUkJwsLCMGbMGLi7W27/JJVKhaSkJIwePRpyuWX2O6KGdaT+julfhsmfpOBskQRiWC9MjA+yyvsUV6owZ792q5tn773LIAfHWH9/cnkPrhZUIKT7ANwZ3XB+UGNEUcSrh3cA0OCeMcMRYWRpfV05+9Kxe3MqHLyCMGFCr2a9p7E2/HIiC33CPfWFFFuDjvQ93hqwv22rOf2tm7lpisnBT3h4OD7//HP968DAQHzzzTf1rjGXg4MDoqOjAQAJCQk4dOgQPvroI6PJ04GBgcjJyTE4lpOTg8DAQP153bGgoCCDa3r37t1gGxQKBRSK+qtI5HK5Vb7BrfVcMq4j9HdciBeev6sLPth+AYt+S8Xw2EB4uThY/H2OXiiAKAKdfV0Q6uNm9Jq6/R0X7I6rBRW4mFeOu7oFNus9S6pUqFRpV26FeLlC3kSxyU6+2n+gXS+qstjf+68nMvHyf09jaFc/fP3kHRZ5piV1hO/x1oT9bVvm9Lep15mc8Hz16lWkpaU1+dVSGo3GIP+mrsTEROzYscPgWFJSkj5HKDIyEoGBgQbXlJSU4MCBAw3mERG1FzOHR6FrgCsKyqvx1qazVnkP3RL3xlZ51aUrdng2s/lJz7oaP26OMjg5NF1lW1/rp9ByCc/Jtfu7nc0sttgzich+7FrkcO7cudi9ezeuXr2KU6dOYe7cuUhOTsa0adMAANOnT8fcuXP117/44ovYsmUL3n//fZw/fx7z58/H4cOH8fzzzwPQrvSYM2cOFi1ahF9++QWnTp3C9OnTERwcjClTptjjIxLZjINMgiVT4yEIwE/HbmDXhbymbzKTvrhhA/V9bndrxVfz6xDlmrChaV264KeoQoWSqpYnpoqiiL2X8gEA+WXVKKqobvEzici+TA5+UlJSsGnTJoNjX3/9NSIjI+Hv749nnnmmwRGbhuTm5mL69OmIiYnByJEjcejQIWzduhWjR48GAGRkZCArK0t//aBBg7BmzRp89tln6NWrF9avX4+NGzeiR48e+mteeeUVvPDCC3jmmWfQv39/lJWVYcuWLXB0tO4KGKLWoG+4F2YM6gQAeO2nUyhX1ljs2TfLq3E+WxvEDDQz+LmcV4YqlbpZ75tTalqBQx1XhQzetVN+lqj0fCW/HNklt+qEXc5recVqIrIvk3N+Fi5ciOHDh+Puu+8GoF2l9dRTT2HGjBno1q0b3n33XQQHB2P+/Pkmv/mqVasaPZ+cnFzv2AMPPIAHHnigwXsEQcDChQuxcOFCk9tB1J68PCYG287k4EZRJd7fdgFvTLLM4oEDtVNeXfxd4WdiIBLk4QhPZzmKKlS4lFuGHiEeZr9vjolbW9QV5u2Mm+XVuHazEt2DzX/PuvbVjvroXM4rQ0KEV4ueSUT2ZfLIz/HjxzFy5Ej967Vr12LAgAH4/PPP8dJLL2HZsmX48ccfrdJIIjKdi0KGf97XEwDw5b40HMsotMhz9VtamJjvA2j/MdItULfDe/PyfnQ5P01talpXmJe2+KIlRn72XtJ+bofajW0v55W1+JlEZF8mBz+FhYUG9XN27dqF8ePH61/3798f1641UYCDiGxiWFc/3NcnBKIIPPvNEWw4dr3F21+Ym++jExfcsqRn3bRXgJvpIz/hFkp6VmtEfdB3d235gMu5nPYiautMDn4CAgL0q7mqq6tx9OhRDBw4UH++tLSUS/+IWpF5d8ch0tcFuaVK/PWHE5jyyV4cTLvZrGfllSpxMVc74jHAzOCnpdtc5Oq3tjBj5MfbMltcnMksRnGlCm4KGab0CQEAXOHID1GbZ3LwM2HCBLz66qv4448/MHfuXDg7O2PIkCH68ydPnkRUVJRVGklE5vNyccDmF4fglXExcFXIcPJ6MR78NAXPfXMEV/PNG73YXzv6ERvopk8mNlW32t3dz2WVNGtPQFM3Na1Lt79XS6e9dFNeAzr7ICZQ+znSb1Zwx3iiNs7k4Oett96CTCbDsGHD8Pnnn+Pzzz+Hg8OtH4JffPEFxowZY5VGElHzOMqlmDU8GjtfHo5HB4RDIgBbzmRj9Ae78NamsyiuaHgpeJmyBgfTbmLVnjR8tvsKAPPyfXS6+LtBLhVQUlWDG2butC6Kon5T0+ZNe1W2aLpv32VtsvPgaB/4uyngqpBBrRGRXsCpL6K2zOTVXr6+vti9ezeKi4vh6uoKqdSw2Ni6dev0W18QUevi56bAP+/ticcTO+Gfv53Drgt5WLUnDf89eh0vjuyCKb1DcD67FKdvFON0ZjFO3ShGWn45bh+ouSvG3+z3dpBJEOXnivPZpTiXVYpQL9O3hyipqkFVbXVnc6a9gjwdIRGA6hoN8sqUZo0a6VSp1Dh0VTtNODjaF4IgIMrPBSeuF+NyXhm6BBivcE1ErZ/ZG5t6eBhfNurt7d3ixhCRdcUEuuGrJ+9Acmou/vnbOVzIKcOCX89iwa/GK0IHeTiiR4gHeoZ4oH8nbwzs3Lz/z+OC3XE+uxRnM0swOi6g6Rtq5dUmO7s7yuDYxLYWdcmlEgR7OuF6YSWu3axoVvBzNKMQVSoN/NwU6OKv/YddlJ9rbfDDkR+itszk4OfJJ5806bovvvii2Y0hItsYHuOPO6N98ePh6/hXUiryy6oR4umEHiHu6BnigR61X76upo+2NCYuyB0/4YbZSc/NqfGjE+bljOuFlci4WYF+ncwP2vbV5vsMivKBIAgAgKjaIOhyLpOeidoyk4Of1atXIyIiAn369GlW0iIRtS4yqQSPDgjHA/1CUVGthoeT9VZr6ld8ZZsb/Ji/0ksn3NsZKVcKcO2meXlGOnv1+T6++mNRfrXBD1d8EbVpJgc/M2fOxPfff4+0tDQ88cQTeOyxxzjVRdQOyKUSeDhZd5s/XfCTXlCB0ioV3BxNC7T0K73MSHbWCfPWFjpsznL30ioVTl7XbmJaN/iJ9ncBoN3iQhRF/YgQEbUtJv/E+89//oOsrCy88sor+PXXXxEWFoYHH3wQW7du5UgQETXK28UBgbVTV6nZpm9yemvkpznBT/MLHR64chNqjYhOPs4I8XTSHw/3doFUIqBMWaOfkiOitsesf+4pFAo88sgjSEpKwtmzZ9G9e3fMmjULnTp1QlkZh4GJqGH6Ss9m5P3ot7YwcS+xuvTBTzNGfvbU7uc1qM6oD6BduRZR+1xOfRG1Xc0e65ZIJBAEAaIoQq1u3m7NRNRx1C12aCp9jZ9mJjwDQHZJFZQ15v2M0tX3ufO24AcAOjPvh6jNMyv4USqV+P777zF69Gh07doVp06dwscff4yMjAzW+CGiRsUFactknM0yfdrrVnVn80d+fF0d4CSXQhSBG4WmJz3nllbhQk4ZBMH4PmZRurwfrvgiarNMTnieNWsW1q5di7CwMDz55JP4/vvv4etb/19FRETG6EZ+UrNLoNaIkEoaTxY2qO7cjJEfQRAQ5u2ECzllSMsv14/YNEW3gWtckDu8jGzlcWvFF2v9ELVVJgc/K1asQHh4ODp37oxdu3Zh165dRq/76aefLNY4Imo/Inxc4CSXolKlRlp+OaL9Gw9GSiproKzdQ8uvGTk/ANA7zBMXcsqwYtdljIj1N2l11t5L9Ze418Xl7kRtn8nBz/Tp07msk4iaTSoREBvkhmMZRTibVdJk8JNbW93Zw0luVnXnul4c1RW/nsjCoauF+Pl4pn5n9oaIoqjfzHRQA/uYRdcGP1nFVShT1sBVYXahfCKyM7OKHBIRtUS3IHccyyjCuawS3NMruNFrb1V3bn6V6RBPJzw/Ihrvbk3F27+dw8hu/o3WGEovqMCNokrIpQLuiDRex8zDWQ5fVwXyy5RIyytHz1DjW/4QUetl3cpmRER1xNUWO0y5XNBkfTB9jZ9mFDis6+khkejk44y8UiX+/fulRq/VVXXuE+4FZ4eG/20Y5adNer6UZ3ryNhG1Hgx+iMhmRnULgINMguPXirD7Yn6j1+pWejVna4u6FDIp3pzUHQDwxZ40XGpklZZuP6/BUY0v5ri1xxeTnonaIgY/RGQzgR6OmD4wAgDw7tbzjY7+tGSl1+3uivXHqG7+qNGImP/LGaPvq9GI+vo+g6ON5/voMOmZqG1j8ENENjVzeBRcHKQ4faMEW05nN3idLuG5OdWdjZl3dxwcZBLsuZSPrWfqv+/ZrBIUVqjg4iBFrzDPRp+lm/Zi8EPUNjH4ISKb8nFV4KkhnQEA721LhVpjfPQnV5/w3PKRH0C71P65odr3fWvTOVRWG1Z91o36DOjsA7m08R+NupGfq/kVqFFrLNI+IrIdBj9EZHNPD4mEp7Mcl/PKseHYDaPX5JTqpr0sM/IDADOHRyPE0wk3iiqxPNkw+bmpJe51hXg6wVEuQbVag+tmVI8motaBwQ8R2Zy7oxwzh0UBAD7cfgHVNYajJ9rqzrpNTS0z8gMATg5SzLu7GwBgxe4rSC/QJixX12hwMO0mgIaLG9YlkQjo7Mu8H6K2isEPEdnF9MRO8HdT4HphJX44lGFwrrhSpQ+ImlvduSFjuwdiSBdfVNdo8NamswCAYxmFqFSp4ePigJgAN5Oeo1/xxeCHqM1h8ENEduHkIMULI7sAAJb9fskgB0e3zN3TufnVnRsiCALenNQdMomA7edy8fv5HOyt3c9rULQvJE3sOaajr/XDDU6J2hwGP0RkNw/1C0OYtxPySpX4KuWq/rh+mbsFp7zqivZ3xVN3RgIAFvx6FrtScwEAg03I99HhBqdEbReDHyKyGweZBHNGdgUALE++jJIqFYBbW1u0tMBhY14Y2QX+bgqkF1TgxPViAKbl++jogp9LuWVNVqsmotaFwQ8R2dWUPiGI9ndFcaUKK3dfAVC3xo91Rn4AwFUhw/9N7KZ/HebthDBvZ5Pvj/R1gSBo85Nulldbo4lEZCUMfojIrqQSAS+P0Y7+rNyThvwyZZ0aP9Yb+QGAe3oF445O2g1M7zRj1AfQ5iyFeDoB4NQXUVvD4IeI7G5s90D0DPFARbUay5Mv19nU1LrBjyAI+OiR3nh2WGe8WDv9Zg5uc0HUNjH4ISK7EwQBfx8bAwD4Zn86zmSWALBcdefGBHk4Ye74bgj0MP+9ovUbnDL4IWpLGPwQUaswpIsvBkR6o7pGg4ybFQAAfxsEPy2hT3rmyA9Rm2LX4Gfx4sXo378/3Nzc4O/vjylTpiA1NbXRe4YPHw5BEOp9TZw4UX/NjBkz6p0fN26ctT8OEbVA3dEfHWtPe7UUNzglapvsGvzs2rULs2fPxv79+5GUlASVSoUxY8agvLzh5MGffvoJWVlZ+q/Tp09DKpXigQceMLhu3LhxBtd9//331v44RNRC/Tp5Y0Ssv/61NZe6W4KuyvP1wkpUqdRNXE1ErYXMnm++ZcsWg9erV6+Gv78/jhw5gqFDhxq9x9vb2+D12rVr4ezsXC/4USgUCAwMtGyDicjq/jamK/64mIdwb2coZJat7mxpPi4O8HCSo7hShbT8cnQLcrd3k4jIBHYNfm5XXKwtNHZ7gNOYVatW4eGHH4aLi4vB8eTkZPj7+8PLywsjRozAokWL4ONjvHqrUqmEUqnUvy4p0SZbqlQqqFQqcz9Gg3TPsuQzqWHsb9uyVH939XPGz7MS4aqQtYm/u86+zjh2rRgXsooR7etk0/fm97htsb9tqzn9beq1gthKSpNqNBrcc889KCoqwp49e0y65+DBgxgwYAAOHDiAO+64Q39cNxoUGRmJy5cv47XXXoOrqytSUlIgldb/l+T8+fOxYMGCesfXrFkDZ2fTi54RUcez5pIEB/IkGB+qxriwVvHj1ECVGpAKgJzLW6gDqKiowKOPPori4mK4uzc8Ettqgp+ZM2di8+bN2LNnD0JDQ02659lnn0VKSgpOnjzZ6HVXrlxBVFQUtm/fjpEjR9Y7b2zkJywsDPn5+Y12nrlUKhWSkpIwevRoyOVyiz2XjGN/21ZH7e/P/kjDu9su4u6egfjgwXibvndTfX76Rgke/PwA1BoR4d7OiPZzQbS/q/6/nX1d4OTQuqcWW5OO+j1uL83p75KSEvj6+jYZ/LSKaa/nn38emzZtwu7du00OfMrLy7F27VosXLiwyWs7d+4MX19fXLp0yWjwo1AooFDUT6yUy+VW+Qa31nPJOPa3bXW0/o4J9AAApBVU2O1zN9Tn3x++DpVa++/bqwUVuFpQge3n8/TnBQEI83JGF39XTOkTgkm9gm3W5raso32P25s5/W3qdXYNfkRRxAsvvIANGzYgOTkZkZGRJt+7bt06KJVKPPbYY01ee/36dRQUFCAoKKglzSUiqke34utyXhk0GhESiWDnFmlVVqvx26lsAMCKxxLg7ijDhZxSXMwt037llKKwQoWMmxXIuFmBPy7lY3RcABzlHAmi9s+uwc/s2bOxZs0a/Pzzz3Bzc0N2tvZ/VA8PDzg5aRMHp0+fjpCQECxevNjg3lWrVmHKlCn1kpjLysqwYMECTJ06FYGBgbh8+TJeeeUVREdHY+zYsbb5YETUYYR5OUEuFVCl0iCzuBKhXq0jT3DrmWyUKWsQ7u2Msd0DIAgCBtXZv0wURRSUV+NiThlmrzmKm+XVOJNZjIQI0xecELVVdk2BW758OYqLizF8+HAEBQXpv3744Qf9NRkZGcjKyjK4LzU1FXv27MFTTz1V75lSqRQnT57EPffcg65du+Kpp55CQkIC/vjjD6NTW0RELSGTStDJR1fssPVscPrfo9cBAPf1DYEg1B+NEgQBvq4KJEb5oG+4FwDgWEaRLZtIZDd2n/ZqSnJycr1jMTExDd7r5OSErVu3trRpREQmi/JzxcXcMlzOLcOwrn72bg6yiiux51I+AGBq36bzKPtGeGL7uRwczSi0dtOIWgUufiQiaqEo/9a1zcWGYzcgisAdkd4I8256Gq5PmHbk52h6kZVbRtQ6MPghImoh3QanrSH4EUUR/z2infK634RRHwDoFeYBqURAdkkVMosqrdk8olaBwQ8RUQvdCn7sn/Nz4noxLueVw1Euwfiepm3x4+wgQ2ygGwBw6os6BAY/REQtpFvunleqRHGlfbc+0I36jO0eCDdH02vR6JKeOfVFHQGDHyKiFnJVyBDo7gjAvlNfyho1fjmRCcC0ROe6+kZ4AgCOXePID7V/DH6IiCxAn/Sca7/g5/dzuSiuVCHQ3RGD69T0MYVu5OfMjRIoa9TNev9tZ7Lx5OpDuFle3az7iWyFwQ8RkQW0hrwfXW2fKX1CIDWz0nS4tzO8XRxQrdbg9I2SZr3/ks3n8fv5XGw4dqNZ9xPZCoMfIiILsPeKr/wyJZJTtft23Z8QYvb9giCgb7gnAOBYM5Ker+aX40q+NvA7l9W84InIVhj8EBFZgC74OZtZgorqGpu//8/HM1GjEdEr1APR/m7NekYfXdJzM4Kf5NRc/Z/PZjL4odaNwQ8RkQXEBbvDSS7FjaJKTP54Ly7mlNr0/XWrvKYmmJfoXFdLtrn4PfXWbvEXc0tRXaNpdjuIrI3BDxGRBXi7OOCLGf3h56bAxdwy3PPxXn1AYm3ns0txNqsEcqmASfHBzX6OrthhVnEVsopNL3ZYUV2D/VcKAAAyiQCVWmwVBR+JGsLgh4jIQhKjfPDbX4bgzmhfVKrU+Nu6E3h53QmrT4NtOKZd3j4yNgBeLg7Nfo5BsUMz6v2kXC5AdY0GoV5O+tEjTn1Ra8bgh4jIgvzcFPjqyTvw0uiukAjA+iPXrToNphaBX05mAWjZlJdOn9qkZ3Pyfn4/r833uSvGH3HB7gCAs0x6blB1jQY1ak4L2hODHyIiC5NKBPxlZBd8+/QAq0+DnS8SkF9WDR8XBwyPafmO8n3NTHoWRVG/ymxErD/igrTBD1d8GVdRXYOR/0rG1BUpEEXR3s3psBj8EBFZyaAoX6tPgx3K09bzuad3MOTSlv9IN7fY4YWcMtwoqoRCJsHAzj4GIz/85V7f8YwiXLtZiRPXipBTorR3czosBj9ERFZkbBpsyn/2IqOgosXPLq5U4dRNbfBj7nYWDYnwuVXs8IwJeTs7a5e4J0b5wMlBimh/V8gkAooqVMgqrrJIm9qTY9eK9H8+n83RMXth8ENEZGW6abDvnh4IPzcFLuSUYcone3Ek/WaLnvvb6WzUiAK6+ruie+2IS0vVLXZ4NL3pqa+dtfk+I2L9AQCOcqm+5hGnvuqrW0AyNdu25RDoFgY/REQ2khjlg1+fvxM9Qtxxs7waj3x+AD8fb/5WELpVXvf2CYYgmLedRWP6mFjvp7hShcO1AdJdMf764/qpL674MiCKokGfMvixHwY/REQ2FOjhiB+fTcSYuABU12jw4trj+HD7BbPzY67kleHYtWIIEHFPryCLttHUFV97LuZDrRER7e+KMG9n/XFd0jNXfBm6drMSBXU2fT3P4MduGPwQEdmYs4MMKx5LwLNDOwMAPtx+EXN+OI4qVdMJxjfLq/Hh9gu4f0UKACDWU4S/m8Ki7esV6gmJgCaLHeryfe66bZWZbuSH016Gjl3TBpN+tX9fl/LKuOTdThj8EBHZgUQiYO6EblhyX0/IJAJ+Pp6JaSsPoKDM+AqgjIIKvPHzaQxasgMfbr+Im+XVCPVywj3hlv/l6aKQITZQG8A0NPWl0Yj6/bzqTnkBQLfakZ+rBRUoU9p+n7PWSteXE3sGwUkuRXWNBlctkPhO5mPwQ0RkRw/fEY6vnrwDbo4yHEkvxJRPDAsinrpejOfXHMXw93bi65R0VKk06BnigY8f7YOkFwcj2MU67eob4Qmg4aTn05nFyC+rhqtChn6dvA3Oebs4INDdEQBwnqM/erpk574RXugaoE0KZ96PfTD4ISKys8HRvtgwazDCvZ1x7WYl7lu+D1/uTcOjn+/HpI/3YNPJLGhEYGhXP6x5egB+eX4w7o4PhswCdX0a0lSxw53ntYUN74z2hYOsfjs49WWoSqXWlw7oE+aJmNptRFK53N0uZPZuABERAdH+rtg4ezCe+fowDqcXYsGvZwFol8nf0ysYfx7SWR9Q2IJuxdfp2mKHCpnU4PzvuimvWONVpbsFueH387lMeq51JrMYNRoRvq4KhHo5IaZ2WpFJz/bB4IeIqJXwdnHAd38egHkbT2Pb2Rzc1ycUT97ZCaFezk3fbGGdaosd3iyvxpnMEv1IEADklylx8noRAGD4bfk+OnFBHgC43F1Hl+/TJ9wTgiDoN5BNtdKeb9Q4Bj9ERK2IQibF0vt7Yamd2yEIAvqEeWLH+VwcyygyCH52peZBFIHuwe4IqM3tuZ1ulOp8dilq1BqrTtG1BbrKzroyArppr4ybFaioroGzA38d21LH/m4kIqIG9Y0wnvezs4FVXnVFeDvD2UEKZY0GVwvKrdfINuK4buQnTNunvq4K+Lo6QBS1+6ORbTH4ISIio3SjFMfqrPiqUWuw+4I22fmu2IaDH4nk1tSOKXuEtWc5JVW4UVQJiQDEh3rojzPp2X4Y/BARkVG6YoeZxVXIrt2k9GhGEUqqauDlLEfvMM9G77+14qtj57Xo8n1iAt3horg1vdU1QBf8cOTH1hj8EBGRUS4KmX5Vkm7qSzflNayrH6SSxvcT68ZtLgDcquysG0nTuZX03LH7xx4Y/BARUYNu3+Fdt4t7Y1NeOvo9vjr4tJd+pddtI2W6wJKFDm2PwQ8RETVIt8rr2LUiZBZV4nx2KSQCMLSL8fo+dcUGukMiaJfG55ZWWbuprVKNWqMvC9Cnzoo5AOga4ApBAPLLqpHfwLYmZB0MfoiIqEG6FV+nbhQj6WwOAO0vcS8XhybvdXKQItJXu/9GR837OZ9diiqVBu6OMnT2NdyLxNlBhnBvbQ0njv7YFoMfIiJqkK7YYXWNBp/tvgKg/i7ujenWwae+dPV9eod7QWIkRyqmNumZlZ5ty67Bz+LFi9G/f3+4ubnB398fU6ZMQWpqaqP3rF69GoIgGHw5OhoW2RJFEW+88QaCgoLg5OSEUaNG4eLFi9b8KERE7ZKu2CEA3CiqBNBwVWdjOvoeX7rNTG/P99GJ5XJ3u7Br8LNr1y7Mnj0b+/fvR1JSElQqFcaMGYPy8sYLYrm7uyMrK0v/lZ6ebnB+6dKlWLZsGVasWIEDBw7AxcUFY8eORVVVx5xzJiJqibqrlPzdFOhuxh5jcR18xdfxOttaGMOkZ/uwaz3tLVu2GLxevXo1/P39ceTIEQwdOrTB+wRBQGBgoNFzoijiww8/xOuvv47JkycDAL7++msEBARg48aNePjhhy33AYiIOoC6W1vcFeMPQWh8iXtduuDnSl4ZKqvVcHKQNnFH+1FYXo0r+dp/zDdUE0lX6PBCThk0GtHo1BhZXqvK+SkuLgYAeHt7N3pdWVkZIiIiEBYWhsmTJ+PMmTP6c2lpacjOzsaoUaP0xzw8PDBgwACkpKRYp+FERO1YrzBtsUOg4V3cG+Lnpt3GQSN2vE08j9eu8urs5wJPZ+MJ4p18nOEgk6BSpca1wgobtq5jazU7qWk0GsyZMweDBw9Gjx49GrwuJiYGX3zxBeLj41FcXIz33nsPgwYNwpkzZxAaGors7GwAQEBAgMF9AQEB+nO3UyqVUCpvLTMsKdEOz6pUKqhUqpZ+ND3dsyz5TGoY+9u22N+2Z6s+d5AATwyKQGpOGQZFepn9frGBbthzqQCnrxeie6BL0ze0Uub295G0AgBAr1CPRu+J8nXBuexSnLlehGD3plfRdRTN+f429VpBFEWxWa2ysJkzZ2Lz5s3Ys2cPQkNDTb5PpVKhW7dueOSRR/DWW29h3759GDx4MDIzMxEUFKS/7sEHH4QgCPjhhx/qPWP+/PlYsGBBveNr1qyBs7Nz8z4QEREBAH5Ol+D3TAnuDNDggc4aezfHZpafleB8sQQPRKpxZ2DDv2q/vSjBoXwJJoSpMTa0VfxKbrMqKirw6KOPori4GO7uDeemtYqRn+effx6bNm3C7t27zQp8AEAul6NPnz64dOkSAOhzgXJycgyCn5ycHPTu3dvoM+bOnYuXXnpJ/7qkpARhYWEYM2ZMo51nLpVKhaSkJIwePRpyudxizyXj2N+2xf62vbbS5zUnsvD7+lOoUHhjwoQ77N2cZjOnvzUaEa8f2wmgBtPGDW40SfzGnjQc2noRonswJkzoZeFWt13N+f7Wzdw0xa7BjyiKeOGFF7BhwwYkJycjMjLS7Geo1WqcOnUKEyZMAABERkYiMDAQO3bs0Ac7JSUlOHDgAGbOnGn0GQqFAgqFot5xuVxulR8o1nouGcf+ti32t+219j6PD9MmTKdml0IqlbX5pF5T+vtSbilKq2rgKJegR6gXZNKGU2zjgj0BABdyy1r136O9mPP9bep1dg1+Zs+ejTVr1uDnn3+Gm5ubPifHw8MDTk5OAIDp06cjJCQEixcvBgAsXLgQAwcORHR0NIqKivDuu+8iPT0dTz/9NADtSrA5c+Zg0aJF6NKlCyIjIzFv3jwEBwdjypQpdvmcREQdWaSvCxxkEpRXq5FxswKdfNtu3o+pjtYucY8P9Ww08AG024AAwNWCClSp1HCUd5wVcfZi1+Bn+fLlAIDhw4cbHP/yyy8xY8YMAEBGRgYkklvfOIWFhfjzn/+M7OxseHl5ISEhAfv27UNcXJz+mldeeQXl5eV45plnUFRUhDvvvBNbtmypVwyRiIisTyaVIDbQDSevF+NsVkmHCH6ONVHfp64AdwU8nOQorlThUm4ZeoR4WLdxZP9pr6YkJycbvP7ggw/wwQcfNHqPIAhYuHAhFi5c2JLmERGRhcQFuePk9WKcyyrBhJ5BTd/Qxt2q7OzVxJXa31kxgW44mHYTqdmlDH5soFXV+SEiovZJt82FJff42nMxHydq985qTcqUNbhQW9PIlJEfoM42Fx2sFpK9MPghIiKr62bhbS6OXyvCY6sO4MFPU5Bd3Lq2Ljp5vQgaEQjxdEKAu2npFl0DdHt8MfixBQY/RERkdbqRjaziKhSWV7f4ee9v026CrazR4KMdF1r8PEvS5fv0NnHUB6i7wSmDH1tg8ENERFbn5ihHhI+2aGxLd3jff6UAf1zMh7R2yfyPh6/jcl5Zi9toKfpk5wb28zKma23wk11SheIKVkm3NgY/RERkE90CWz71JYqiftTnkTvCMKqbP9QaEf/a1jpGf0RRxPFrtcnO4U0nO+u4O8oR4qkt8XI+23J5UWQcgx8iIrIJfdJzC4KfXRfycOhqIRQyCV4Y0QUvj42BIAD/O5WFk7UbidrT9cJK5JdVQy4VGq3qbEwMk55thsEPERHZRFxQy1Z8aUd9tCM8fxoYgQB3R8QGuuPe3iEAgHe3plqmoS1wtHaJe1ywh9nFCnXBz3nm/Vgdgx8iIrKJbrUjIZdyy6CsUZt9/9YzOTh1oxguDlLMHB6lP/7X0V0hlwr442I+9l7Kt1h7m6M5+T46THq2HQY/RERkE8EejvBwkqNGI+JSrnkJymqNiH8laUd2nrwzEj6ut/ZjDPN2xrQBEQCApVvOm1RA11qO1dYdMrW+T126kZ8L2aV2/QwdAYMfIiKyCUEQmj31telkJi7klMHdUYanh3Sud/75EdFwdpDixPVibDmdbZH2mqtKpcbZzGIAQF8zkp11Ovu6QiYRUKqsQWYrq13U3jD4ISIim9ElAa/8Iw25pab9glepNfggSZvr8+ywKHg41d+529dVoQ+K3t2Wihq1xkItbpwoikjNLsWKXZfx2MoDUKlF+Lo6INTLyexnOcgk6Oyn3fcslSu+rIrBDxER2cz0xE7wd1MgNacUD326HzeKKpu856ej13G1oAI+Lg6YMahTg9f9eUgkvJzluJJXjv8evW7BVhsqV9Yg6WwOXttwCoOX/I6xH+7Gks3ncThdm+x8b58QCILQrGfH1JYDYNKzdTH4ISIimwn3cca65xIR4umEtPxyPLB8H9Lyyxu8XlmjxrIdlwAAM4dHwUXR8H7cbo5yzL4rGgDw4faLqFKZn1TdkMpqNVanpOM/ZyXov3gn/vz1Yaw5kIHM4iooZBIMj/HDgnu6Y9ffh+P/JsY1+32Y9Gwbdt3VnYiIOp4IHxesn5mIaSsP4EpeOR5YkYJvn74DsYH16+J8fyADN4oqEejuiMcGRjT57McGRuCLPWnILK7CNynp+PPQ+vlB5lLWqPH014ew91IBtGMGIsK9nXFXjB+Gx/ojsbOP2cvaGxLDPb5sgiM/RERkc0EeTvjx2UR0C3JHfpkSD326H8dv26G9slqNj3deBgC8MDLapADDUS7FnNFdAQD/Sb6EkqqWbRWh1oj46w/HsfdSAZwdpJgSocbWvwzGrr8Px4LJPXBXjL/FAh/g1oqvy3llUNkob6kjYvBDRER24euqwNo/D0TfcE8UV6ow7fP9SLlcoD//VcpV5JcpEebthAcSwkx+7tS+oeji74qiChU+23Wl2e0TRRFv/Hwav53Khlwq4JNHe+OuYBGd/VyandPTlFAvJ7gqZFCpRVzJa3g6kFqGwQ8REdmNh7Mc3zw1AIOifFBercaMLw9i5/lclFSpsGKXdtRnzsiucJCZ/utKKhHw8tgYAMCqPaavKrvdB9sv4rsDGRAE4MOH+mBwlE+znmMOQRDQNcAVAPf4siYGP0REZFcuChm+mNEfo7r5Q1mjwZ+/Pozn1xxDUYUK0f6umNInxOxnjokLQO8wT1Sq1Pj490tm3796bxqW7bgIAHhrcg9MjA8y+xnNpVvxxbwf62HwQ0REducol2L5Ywm4p1cwajQidl/IAwC8NLorpBLzp5gEQcA/xsUCAL47kIHXN55CTolpI0A/H7+B+b+e1b+/KYnWlhRTO/JzgRucWg2DHyIiahXkUgk+eKg3HrlDm98TH+qBcd0Dm/28xCgfPNw/DGqNiG/3Z2Do0p1Y/Ns5FJZXN3jPrgt5+NuPJwAAjydG4IUR0c1+/+ZirR/r41J3IiJqNaQSAf+8tyfu6xuKmEA3SJox6lPXkqnxmNw7BO9tS8WR9EJ8uvsKvjuQgaeHROKpOyPh5nirWvSxjEI8980R1GhE3NMrGG9O6m61xObG6Gr9XC+sRJmyBq6N1Dai5uHIDxERtSqCIKB/J2+4O9bfxqI5EqN8sP65RHw5oz/igtxRpqzBh9svYujSnfhs92VUqdS4mFOKJ1YfQqVKjaFd/fDeA71aHHg1l5eLA4I9HAEAB9MKmri6cRXVNfhmfzoKypSWaFq7weCHiIjaPUEQcFesPza9cCf+82hfdPZzQWGFCv/87TyGLt2Jx1YdQFGFCr3DPLHisb5mrS6zhjG1030bj2W26Dkfbb+IeRtP44nVh1Bdw7pBOgx+iIiow5BIBEyMD8K2OUPx7v3xCPF0Qm6pEjklSkT7u+LLGf3h7GD/aaZ7a1e4bTubjTJlTbOeoVJr8N+jNwAAJ68XY+mW8xZrX1vH4IeIiDocmVSCB/qF4feXh+Gtyd1xf0IovnnqDni5ONi7aQC0yd6dfV1QpdJg6+nsZj1j94U85Jcp4VRbgXrlnjRsP5tjyWa2WQx+iIiow1LIpPhTYie890AvBHk42bs5eoIgYHJv7ejPxuM3mvWM9Ue0O9tPGxCOJwdHAgBeXn8CmUWVlmlkG8bgh4iIqBWa0icYALD3Ur7ZVaoLy6ux/Zx2lGdqQiheHR+L+FAPFFWo8Jfvj6Gmg+8bxuCHiIioFYrwcUHfcE9oRODXE1lm3fvLiUyo1CJ6hLijW5A7HGQS/PuRPnBTyHA4vRAfbL9gpVa3DQx+iIiIWind1h4bj5k39aWb8rq/b6j+WISPCxZP7QkA+CT5sr6KdkfE4IeIiKiVmtgzCDKJgFM3inEpt8yke85nl+DUjWLIpQLu6W24L9rd8cGYNiAcogi89ONx5Jq45Ud7w+CHiIiolfJxVWBoVz8A2j3HTPHf2lGfkbEB8Dayem3e3XGIDXRDflk15vxwHGqN2OQzNRoRRzMK281mqwx+iIiIWjH91NfxGxDFxgMVlVqDDbWFEe9PCDV6jaNcio8f7QsnuRT7LhfgPzuN73oviiJO3yjGP387h0FLfsd9n+zD2A93Y9rK/fjjYl6TbWnN7F/JiYiIiBo0ulsAXBykuHazEkczCpEQ4d3gtbraPr6uDhgW49fgddH+rlg0pQf+tu4EPtx+AXdEemNgZx8AQHpBOX45nomfT2QaTLW5KWSoUKmx91IB9l4qQPdgdzw3LArjewRCJm1bYykMfoiIiFoxJwcpxvYIxE9Hb2DDsRuNBj+6ROcpvUMgbyIgmZoQin2XC/Dfo9fx4tpjeGZoFDadzMSxjCL9NQ4yCUZ188fk3iEYHuOHvFIlVv6Rhh8OXcOZzBK88P0xhHs745mhnXF/QigcawsqtnZ2DdUWL16M/v37w83NDf7+/pgyZQpSU1Mbvefzzz/HkCFD4OXlBS8vL4waNQoHDx40uGbGjBkQBMHga9y4cdb8KERERFaj2+7ifyezGtyj6/baPqZYOLk7ovxckFOixFubzuJYRhEkAjCkiy/evT8eh18fhU+mJWBs90AoZFKEejlj/j3dse/VEZgzqgu8nOXIuFmB1zeexp3v/I7/7LyE4kqVZT60Fdk1+Nm1axdmz56N/fv3IykpCSqVCmPGjEF5eXmD9yQnJ+ORRx7Bzp07kZKSgrCwMIwZMwY3bhgmgo0bNw5ZWVn6r++//97aH4eIiMgqBkX5ws9NgcIKVYNL1HW1fboHa2v7mMJFIcN/pvVFiKcT4kM9MO/uOOyfOxLfPDUAD/QLg7uj3Oh9Xi4OmDOqK/a+OgLzJ8UhxNMJ+WXVeHdrKsZ8sAv5rXwXebtOe23ZssXg9erVq+Hv748jR45g6NChRu/57rvvDF6vXLkS//3vf7Fjxw5Mnz5df1yhUCAwMNDyjSYiIrIxqUTAPb2CsWpPGjYev4FRcQH1rtHX9jFx1EcnNtAde18d0ax2OTvIMGNwJKYNjMD/TmZh6ZbzyCyuwi/HM/HknZHNeqYttKoMpeLiYgCAt3fD85m3q6iogEqlqndPcnIy/P39ERMTg5kzZ6KgoMCibSUiIrIl3dRX0tkclFYZTi3Vre0z+bbaPrYgl0owpU8I/jy0MwBg08lMm7fBHK0m4Vmj0WDOnDkYPHgwevToYfJ9//jHPxAcHIxRo0bpj40bNw733XcfIiMjcfnyZbz22msYP348UlJSIJXWT8ZSKpVQKm8N0ZWUlAAAVCoVVCrLzV3qnmXJZ1LD2N+2xf62Pfa5bdm7v7v6OaGzrwuu5JfjfyduYGrfW0HOukMZAIDhXf3g5iDYrY1juvlh4SbgaEYRruaVIMSz+ZvFNqe/Tb1WEFvJQv2ZM2di8+bN2LNnD0JDTRuyW7JkCZYuXYrk5GTEx8c3eN2VK1cQFRWF7du3Y+TIkfXOz58/HwsWLKh3fM2aNXB2djb9QxAREVnRtusC/ndNiq4eGsyO0yY+qzXAm0elKFUJeDpGjZ7e9v21/u8zElwqkeCecDVGhti2LRUVFXj00UdRXFwMd/eG855aRfDz/PPP4+eff8bu3bsRGWnaHOF7772HRYsWYfv27ejXr1+T1/v5+WHRokV49tln650zNvITFhaG/Pz8RjvPXCqVCklJSRg9ejTkcuNJZGQ57G/bYn/bHvvctlpDf18rrMCIf+2BIAB/vDwUAe6O+D01D89+ewzeLnLs+fuwJpe4W9uag9fw5q/n0D3YDRtnJjb7Oc3p75KSEvj6+jYZ/Nh12ksURbzwwgvYsGEDkpOTTQ58li5dirfffhtbt241KfC5fv06CgoKEBQUZPS8QqGAQqGod1wul1vlG9xazyXj2N+2xf62Pfa5bdmzvzv7e6BfhBcOpxdiy9k8PD2kMzYe1+74fm+fUDg71v9dZmt39wrBwv+dx5nMUlwvrkakr0uLnmdOf5t6nV3Dw9mzZ+Pbb7/FmjVr4ObmhuzsbGRnZ6OyslJ/zfTp0zF37lz963feeQfz5s3DF198gU6dOunvKSvTVqEsKyvD3//+d+zfvx9Xr17Fjh07MHnyZERHR2Ps2LE2/4xERESWpNvuYsOxGwa1fcxd5WUtPq4KDIrSVovedKJ1Jj7bNfhZvnw5iouLMXz4cAQFBem/fvjhB/01GRkZyMrKMrinuroa999/v8E97733HgBAKpXi5MmTuOeee9C1a1c89dRTSEhIwB9//GF0dIeIiKgt0e30fiazBO8npZpd28cWJvUKBgBsOpnVxJX2Yfdpr6YkJycbvL569Wqj1zs5OWHr1q0taBUREVHr5eXigOEx/th+Lgff7teu8motoz46Y+MC8X/SU0jNKcWFnFJ0DXCzd5MMtKo6P0RERNQ0Xc0fAHar7dMYD2c5hnbRbqzaGqe+GPwQERG1MSO7+cNVoZ28GRHrD28XBzu3qL66U1+tYGG5AQY/REREbYyjXIrHBkZAJhHw5ODWuY3EqLgAKGQSXMkvx5nMEns3xwCDHyIiojbolbExODl/DAZ09rF3U4xyVcgwItYfQOtLfGbwQ0RE1AZJJAKcHVrNLlVG3R2vm/rKbFVTXwx+iIiIyCpGxPrD2UGK64WVOH6tyN7N0WPwQ0RERFbh5CDFqG4BAFrX1BeDHyIiIrKau+O1W0v972QWNJrWMfXF4IeIiIisZliMH9wcZcguqcLh9EJ7NwcAgx8iIiKyIoVMijFxgQCAX1tJwUMGP0RERGRVk3ppp742n85CjVpj59Yw+CEiIiIrGxztCy9nOfLLqrH/yk17N4fBDxEREVmXXCrBuB7aqa9NJ+0/9cXgh4iIiKxuUm3Bwy1nslFdY9+pLwY/REREZHUDOvvA11WBogoV9l7Kt2tbGPwQERGR1UklAib2rF31ZeepLwY/REREZBN399JOfW07k4Mqldpu7WDwQ0RERDaREO6FIA9HlClrsOtCnt3aweCHiIiIbEIiETCxZxBcFTLkllTZrR0yu70zERERdTjPj4jGy2Nj4CiX2q0NDH6IiIjIZjydHezdBE57ERERUcfC4IeIiIg6FAY/RERE1KEw+CEiIqIOhcEPERERdSgMfoiIiKhDYfBDREREHQqDHyIiIupQGPwQERFRh8Lgh4iIiDoUBj9ERETUoTD4ISIiog6FwQ8RERF1KNzV3QhRFAEAJSUlFn2uSqVCRUUFSkpKIJfLLfpsqo/9bVvsb9tjn9sW+9u2mtPfut/but/jDWHwY0RpaSkAICwszM4tISIiInOVlpbCw8OjwfOC2FR41AFpNBpkZmbCzc0NgiBY7LklJSUICwvDtWvX4O7ubrHnknHsb9tif9se+9y22N+21Zz+FkURpaWlCA4OhkTScGYPR36MkEgkCA0Ntdrz3d3d+T+ODbG/bYv9bXvsc9tif9uWuf3d2IiPDhOeiYiIqENh8ENEREQdCoMfG1IoFHjzzTehUCjs3ZQOgf1tW+xv22Of2xb727as2d9MeCYiIqIOhSM/RERE1KEw+CEiIqIOhcEPERERdSgMfoiIiKhDYfBjQ//5z3/QqVMnODo6YsCAATh48KC9m9Qu7N69G5MmTUJwcDAEQcDGjRsNzouiiDfeeANBQUFwcnLCqFGjcPHiRfs0th1YvHgx+vfvDzc3N/j7+2PKlClITU01uKaqqgqzZ8+Gj48PXF1dMXXqVOTk5NipxW3b8uXLER8fry/0lpiYiM2bN+vPs6+tZ8mSJRAEAXPmzNEfY39b1vz58yEIgsFXbGys/ry1+pvBj4388MMPeOmll/Dmm2/i6NGj6NWrF8aOHYvc3Fx7N63NKy8vR69evfCf//zH6PmlS5di2bJlWLFiBQ4cOAAXFxeMHTsWVVVVNm5p+7Br1y7Mnj0b+/fvR1JSElQqFcaMGYPy8nL9NX/961/x66+/Yt26ddi1axcyMzNx33332bHVbVdoaCiWLFmCI0eO4PDhwxgxYgQmT56MM2fOAGBfW8uhQ4fw6aefIj4+3uA4+9vyunfvjqysLP3Xnj179Oes1t8i2cQdd9whzp49W/9arVaLwcHB4uLFi+3YqvYHgLhhwwb9a41GIwYGBorvvvuu/lhRUZGoUCjE77//3g4tbH9yc3NFAOKuXbtEUdT2r1wuF9etW6e/5ty5cyIAMSUlxV7NbFe8vLzElStXsq+tpLS0VOzSpYuYlJQkDhs2THzxxRdFUeT3tjW8+eabYq9evYyes2Z/c+THBqqrq3HkyBGMGjVKf0wikWDUqFFISUmxY8vav7S0NGRnZxv0vYeHBwYMGMC+t5Di4mIAgLe3NwDgyJEjUKlUBn0eGxuL8PBw9nkLqdVqrF27FuXl5UhMTGRfW8ns2bMxceJEg34F+L1tLRcvXkRwcDA6d+6MadOmISMjA4B1+5sbm9pAfn4+1Go1AgICDI4HBATg/PnzdmpVx5CdnQ0ARvted46aT6PRYM6cORg8eDB69OgBQNvnDg4O8PT0NLiWfd58p06dQmJiIqqqquDq6ooNGzYgLi4Ox48fZ19b2Nq1a3H06FEcOnSo3jl+b1vegAEDsHr1asTExCArKwsLFizAkCFDcPr0aav2N4MfImq22bNn4/Tp0wZz9GR5MTExOH78OIqLi7F+/Xo8/vjj2LVrl72b1e5cu3YNL774IpKSkuDo6Gjv5nQI48eP1/85Pj4eAwYMQEREBH788Uc4OTlZ7X057WUDvr6+kEql9TLUc3JyEBgYaKdWdQy6/mXfW97zzz+PTZs2YefOnQgNDdUfDwwMRHV1NYqKigyuZ583n4ODA6Kjo5GQkIDFixejV69e+Oijj9jXFnbkyBHk5uaib9++kMlkkMlk2LVrF5YtWwaZTIaAgAD2t5V5enqia9euuHTpklW/vxn82ICDgwMSEhKwY8cO/TGNRoMdO3YgMTHRji1r/yIjIxEYGGjQ9yUlJThw4AD7vplEUcTzzz+PDRs24Pfff0dkZKTB+YSEBMjlcoM+T01NRUZGBvvcQjQaDZRKJfvawkaOHIlTp07h+PHj+q9+/fph2rRp+j+zv62rrKwMly9fRlBQkHW/v1uULk0mW7t2rahQKMTVq1eLZ8+eFZ955hnR09NTzM7OtnfT2rzS0lLx2LFj4rFjx0QA4r/+9S/x2LFjYnp6uiiKorhkyRLR09NT/Pnnn8WTJ0+KkydPFiMjI8XKyko7t7xtmjlzpujh4SEmJyeLWVlZ+q+Kigr9Nc8995wYHh4u/v777+Lhw4fFxMREMTEx0Y6tbrteffVVcdeuXWJaWpp48uRJ8dVXXxUFQRC3bdsmiiL72trqrvYSRfa3pf3tb38Tk5OTxbS0NHHv3r3iqFGjRF9fXzE3N1cURev1N4MfG/r3v/8thoeHiw4ODuIdd9wh7t+/395Nahd27twpAqj39fjjj4uiqF3uPm/ePDEgIEBUKBTiyJEjxdTUVPs2ug0z1tcAxC+//FJ/TWVlpThr1izRy8tLdHZ2Fu+9914xKyvLfo1uw5588kkxIiJCdHBwEP38/MSRI0fqAx9RZF9b2+3BD/vbsh566CExKChIdHBwEENCQsSHHnpIvHTpkv68tfpbEEVRbNnYEREREVHbwZwfIiIi6lAY/BAREVGHwuCHiIiIOhQGP0RERNShMPghIiKiDoXBDxEREXUoDH6IiIioQ2HwQ0RkAkEQsHHjRns3g4gsgMEPEbV6M2bMgCAI9b7GjRtn76YRURsks3cDiIhMMW7cOHz55ZcGxxQKhZ1aQ0RtGUd+iKhNUCgUCAwMNPjy8vICoJ2SWr58OcaPHw8nJyd07twZ69evN7j/1KlTGDFiBJycnODj44NnnnkGZWVlBtd88cUX6N69OxQKBYKCgvD8888bnM/Pz8e9994LZ2dndOnSBb/88ot1PzQRWQWDHyJqF+bNm4epU6fixIkTmDZtGh5++GGcO3cOAFBeXo6xY8fCy8sLhw4dwrp167B9+3aD4Gb58uWYPXs2nnnmGZw6dQq//PILoqOjDd5jwYIFePDBB3Hy5ElMmDAB06ZNw82bN236OYnIAlq8NSoRkZU9/vjjolQqFV1cXAy+3n77bVEUtTvNP/fccwb3DBgwQJw5c6YoiqL42WefiV5eXmJZWZn+/P/+9z9RIpGI2dnZoiiKYnBwsPh///d/DbYBgPj666/rX5eVlYkAxM2bN1vscxKRbTDnh4jahLvuugvLly83OObt7a3/c2JiosG5xMREHD9+HABw7tw59OrVCy4uLvrzgwcPhkajQWpqKgRBQGZmJkaOHNloG+Lj4/V/dnFxgbu7O3Jzc5v7kYjIThj8EFGb4OLiUm8aylKcnJxMuk4ulxu8FgQBGo3GGk0iIitizg8RtQv79++v97pbt24AgG7duuHEiRMoLy/Xn9+7dy8kEgliYmLg5uaGTp06YceOHTZtMxHZB0d+iKhNUCqVyM7ONjgmk8ng6+sLAFi3bh369euHO++8E9999x0OHjyIVatWAQCmTZuGN998E48//jjmz5+PvLw8vPDCC/jTn/6EgIAAAMD8+fPx3HPPwd/fH+PHj0dpaSn27t2LF154wbYflIisjsEPEbUJW7ZsQVBQkMGxmJgYnD9/HoB2JdbatWsxa9YsBAUF4fvvv0dcXBwAwNnZGVu3bsWLL76I/v37w9nZGVOnTsW//vUv/bMef/xxVFVV4YMPPsDLL78MX19f3H///bb7gERkM4IoiqK9G0FE1BKCIGDDhg2YMmWKvZtCRG0Ac36IiIioQ2HwQ0RERB0Kc36IqM3j7D0RmYMjP0RERNShMPghIiKiDoXBDxEREXUoDH6IiIioQ2HwQ0RERB0Kgx8iIiLqUBj8EBERUYfC4IeIiIg6FAY/RERE1KH8P6FX0Eyi7XpuAAAAAElFTkSuQmCC\n"},"metadata":{}},{"name":"stdout","text":"✅ 模型已保存为 simple_rass_model.pth\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}